#!/usr/bin/env python3
"""
LBSæƒé‡éªŒè¯å’Œå¯è§†åŒ–æµ‹è¯•è„šæœ¬

æµ‹è¯•å·²ä¼˜åŒ–çš„LBSæƒé‡åœ¨ä¸åŒå¸§ä¸Šçš„é‡å»ºè´¨é‡ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœã€‚
"""

import numpy as np
import open3d as o3d
import matplotlib.pyplot as plt
from pathlib import Path
import json
from tqdm import tqdm
import argparse
import time

from Skinning import InverseMeshCanonicalizer


class LBSValidator:
    def __init__(self, skeleton_data_dir, mesh_folder_path, weights_path, reference_frame_idx=5):
        """
        åˆå§‹åŒ–LBSéªŒè¯å™¨
        
        Args:
            skeleton_data_dir: éª¨éª¼æ•°æ®æ–‡ä»¶å¤¹
            mesh_folder_path: ç½‘æ ¼æ–‡ä»¶å¤¹
            weights_path: æƒé‡æ–‡ä»¶è·¯å¾„
            reference_frame_idx: å‚è€ƒå¸§ç´¢å¼•
        """
        self.skeleton_data_dir = Path(skeleton_data_dir)
        self.mesh_folder_path = Path(mesh_folder_path)
        self.weights_path = Path(weights_path)
        self.reference_frame_idx = reference_frame_idx
        
        # åˆå§‹åŒ–canonicalizer
        self.canonicalizer = InverseMeshCanonicalizer(
            skeleton_data_dir=skeleton_data_dir,
            reference_frame_idx=reference_frame_idx
        )
        
        # åŠ è½½ç½‘æ ¼åºåˆ—
        self.canonicalizer.load_mesh_sequence(mesh_folder_path)
        
        # åŠ è½½æƒé‡
        if not self.canonicalizer.load_skinning_weights(weights_path):
            raise ValueError(f"æ— æ³•åŠ è½½æƒé‡æ–‡ä»¶: {weights_path}")
        
        self.results = {}
        
    def test_single_frame(self, frame_idx, verbose=False):
        """
        æµ‹è¯•å•ä¸ªå¸§çš„é‡å»ºè´¨é‡
        
        Args:
            frame_idx: å¸§ç´¢å¼•
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            frame_result: å¸§æµ‹è¯•ç»“æœå­—å…¸
        """
        if frame_idx >= len(self.canonicalizer.mesh_files):
            return None
            
        # åŠ è½½ç›®æ ‡ç½‘æ ¼
        target_mesh = o3d.io.read_triangle_mesh(str(self.canonicalizer.mesh_files[frame_idx]))
        target_vertices = np.asarray(target_mesh.vertices)
        
        # è·å–rest poseé¡¶ç‚¹
        rest_vertices = self.canonicalizer.rest_pose_vertices
        
        # å½’ä¸€åŒ–å¤„ç†
        if frame_idx not in self.canonicalizer.frame_normalization_params:
            self.canonicalizer.frame_normalization_params[frame_idx] = \
                self.canonicalizer.compute_mesh_normalization_params(target_mesh)
        
        target_vertices_norm = self.canonicalizer.normalize_mesh_vertices(
            target_vertices, 
            self.canonicalizer.frame_normalization_params[frame_idx]
        )
        rest_vertices_norm = self.canonicalizer.normalize_mesh_vertices(
            rest_vertices, 
            self.canonicalizer.frame_normalization_params[self.reference_frame_idx]
        )
        
        # è®¡ç®—ç›¸å¯¹å˜æ¢
        target_transforms = self.canonicalizer.transforms[frame_idx]
        rest_transforms = self.canonicalizer.transforms[self.reference_frame_idx]
        
        relative_transforms = np.zeros_like(target_transforms)
        for j in range(self.canonicalizer.num_joints):
            if np.linalg.det(rest_transforms[j][:3, :3]) > 1e-6:
                rest_inv = np.linalg.inv(rest_transforms[j])
                relative_transforms[j] = target_transforms[j] @ rest_inv
            else:
                relative_transforms[j] = np.eye(4)
        
        # ä½¿ç”¨LBSé¢„æµ‹é¡¶ç‚¹ä½ç½®
        start_time = time.time()
        predicted_vertices = self.canonicalizer.apply_lbs_transform(
            rest_vertices_norm, 
            self.canonicalizer.skinning_weights, 
            relative_transforms
        )
        lbs_time = time.time() - start_time
        
        # è®¡ç®—è¯¯å·®æŒ‡æ ‡
        vertex_errors = np.linalg.norm(predicted_vertices - target_vertices_norm, axis=1)
        
        frame_result = {
            'frame_idx': frame_idx,
            'mean_error': np.mean(vertex_errors),
            'max_error': np.max(vertex_errors),
            'min_error': np.min(vertex_errors),
            'std_error': np.std(vertex_errors),
            'median_error': np.median(vertex_errors),
            'rmse': np.sqrt(np.mean(vertex_errors**2)),
            'lbs_time': lbs_time,
            'num_vertices': len(target_vertices),
            'vertex_errors': vertex_errors,
            'predicted_vertices': predicted_vertices,
            'target_vertices_norm': target_vertices_norm,
            'relative_transforms': relative_transforms
        }
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°è¯¯å·®
        frame_result['p95_error'] = np.percentile(vertex_errors, 95)
        frame_result['p99_error'] = np.percentile(vertex_errors, 99)
        
        # è®¡ç®—è¯¯å·®åˆ†å¸ƒ
        frame_result['error_histogram'] = np.histogram(vertex_errors, bins=50)
        
        if verbose:
            print(f"å¸§ {frame_idx:3d}: å¹³å‡è¯¯å·®={frame_result['mean_error']:.6f}, "
                  f"RMSE={frame_result['rmse']:.6f}, "
                  f"æœ€å¤§è¯¯å·®={frame_result['max_error']:.6f}, "
                  f"æ—¶é—´={lbs_time:.3f}s")
        
        return frame_result
    
    def test_frame_range(self, start_frame=0, end_frame=None, step=1, verbose=True):
        """
        æµ‹è¯•å¸§èŒƒå›´çš„é‡å»ºè´¨é‡
        
        Args:
            start_frame: èµ·å§‹å¸§
            end_frame: ç»“æŸå¸§ï¼ŒNoneè¡¨ç¤ºåˆ°æœ€åä¸€å¸§
            step: æ­¥é•¿
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            results: æµ‹è¯•ç»“æœå­—å…¸
        """
        if end_frame is None:
            end_frame = len(self.canonicalizer.mesh_files)
        
        end_frame = min(end_frame, len(self.canonicalizer.mesh_files))
        frame_indices = list(range(start_frame, end_frame, step))
        
        print(f"æµ‹è¯•å¸§èŒƒå›´: {start_frame} åˆ° {end_frame-1} (æ­¥é•¿={step})")
        print(f"æ€»å…±æµ‹è¯• {len(frame_indices)} å¸§")
        
        results = {
            'frame_results': {},
            'summary': {},
            'test_params': {
                'start_frame': start_frame,
                'end_frame': end_frame,
                'step': step,
                'total_frames': len(frame_indices),
                'reference_frame': self.reference_frame_idx
            }
        }
        
        # æµ‹è¯•å„å¸§
        total_errors = []
        total_times = []
        
        for frame_idx in tqdm(frame_indices, desc="æµ‹è¯•å¸§é‡å»ºè´¨é‡"):
            frame_result = self.test_single_frame(frame_idx, verbose=False)
            if frame_result is not None:
                results['frame_results'][frame_idx] = frame_result
                total_errors.append(frame_result['mean_error'])
                total_times.append(frame_result['lbs_time'])
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if total_errors:
            results['summary'] = {
                'overall_mean_error': np.mean(total_errors),
                'overall_std_error': np.std(total_errors),
                'overall_max_error': np.max(total_errors),
                'overall_min_error': np.min(total_errors),
                'overall_median_error': np.median(total_errors),
                'average_lbs_time': np.mean(total_times),
                'total_lbs_time': np.sum(total_times),
                'frames_per_second': len(total_times) / np.sum(total_times) if np.sum(total_times) > 0 else 0
            }
            
            if verbose:
                print(f"\nğŸ“Š æ€»ä½“æµ‹è¯•ç»“æœ:")
                print(f"å¹³å‡é‡å»ºè¯¯å·®: {results['summary']['overall_mean_error']:.6f} Â± {results['summary']['overall_std_error']:.6f}")
                print(f"è¯¯å·®èŒƒå›´: [{results['summary']['overall_min_error']:.6f}, {results['summary']['overall_max_error']:.6f}]")
                print(f"å¹³å‡LBSæ—¶é—´: {results['summary']['average_lbs_time']:.3f}s")
                print(f"å¤„ç†é€Ÿåº¦: {results['summary']['frames_per_second']:.1f} FPS")
        
        self.results = results
        return results
    
    def analyze_error_distribution(self, frame_idx_list=None):
        """
        åˆ†æè¯¯å·®åˆ†å¸ƒ
        
        Args:
            frame_idx_list: è¦åˆ†æçš„å¸§åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å·²æµ‹è¯•çš„å¸§
        """
        if not self.results or 'frame_results' not in self.results:
            print("è¯·å…ˆè¿è¡Œæµ‹è¯•")
            return
        
        if frame_idx_list is None:
            frame_idx_list = list(self.results['frame_results'].keys())
        
        print(f"\nğŸ“ˆ è¯¯å·®åˆ†å¸ƒåˆ†æ (åŸºäº {len(frame_idx_list)} å¸§):")
        
        all_errors = []
        for frame_idx in frame_idx_list:
            if frame_idx in self.results['frame_results']:
                frame_result = self.results['frame_results'][frame_idx]
                all_errors.extend(frame_result['vertex_errors'])
        
        if not all_errors:
            print("æ²¡æœ‰å¯åˆ†æçš„è¯¯å·®æ•°æ®")
            return
        
        all_errors = np.array(all_errors)
        
        print(f"æ€»é¡¶ç‚¹æ•°: {len(all_errors):,}")
        print(f"è¯¯å·®ç»Ÿè®¡:")
        print(f"  å¹³å‡: {np.mean(all_errors):.6f}")
        print(f"  æ ‡å‡†å·®: {np.std(all_errors):.6f}")
        print(f"  ä¸­ä½æ•°: {np.median(all_errors):.6f}")
        print(f"  25%åˆ†ä½: {np.percentile(all_errors, 25):.6f}")
        print(f"  75%åˆ†ä½: {np.percentile(all_errors, 75):.6f}")
        print(f"  95%åˆ†ä½: {np.percentile(all_errors, 95):.6f}")
        print(f"  99%åˆ†ä½: {np.percentile(all_errors, 99):.6f}")
        print(f"  æœ€å¤§å€¼: {np.max(all_errors):.6f}")
        
        # åˆ†æé«˜è¯¯å·®é¡¶ç‚¹æ¯”ä¾‹
        thresholds = [0.01, 0.02, 0.05, 0.1]
        print(f"\né«˜è¯¯å·®é¡¶ç‚¹æ¯”ä¾‹:")
        for threshold in thresholds:
            ratio = np.sum(all_errors > threshold) / len(all_errors) * 100
            print(f"  è¯¯å·® > {threshold:.3f}: {ratio:.2f}%")
    
    def generate_visualization_plots(self, output_dir="output/lbs_validation", 
                                   frame_idx_list=None, max_frames_to_plot=20):
        """
        ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            frame_idx_list: è¦å¯è§†åŒ–çš„å¸§åˆ—è¡¨
            max_frames_to_plot: æœ€å¤§ç»˜åˆ¶å¸§æ•°
        """
        if not self.results or 'frame_results' not in self.results:
            print("è¯·å…ˆè¿è¡Œæµ‹è¯•")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if frame_idx_list is None:
            frame_idx_list = list(self.results['frame_results'].keys())
        
        # é™åˆ¶ç»˜åˆ¶çš„å¸§æ•°
        if len(frame_idx_list) > max_frames_to_plot:
            step = len(frame_idx_list) // max_frames_to_plot
            frame_idx_list = frame_idx_list[::step]
        
        print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨åˆ°: {output_path}")
        
        # 1. è¯¯å·®éšå¸§å˜åŒ–çš„è¶‹åŠ¿å›¾
        plt.figure(figsize=(15, 10))
        
        # å‡†å¤‡æ•°æ®
        frames = []
        mean_errors = []
        max_errors = []
        rmse_errors = []
        
        for frame_idx in sorted(frame_idx_list):
            if frame_idx in self.results['frame_results']:
                result = self.results['frame_results'][frame_idx]
                frames.append(frame_idx)
                mean_errors.append(result['mean_error'])
                max_errors.append(result['max_error'])
                rmse_errors.append(result['rmse'])
        
        # å­å›¾1: è¯¯å·®è¶‹åŠ¿
        plt.subplot(2, 3, 1)
        plt.plot(frames, mean_errors, 'b-o', label='Mean Error', markersize=4)
        plt.plot(frames, rmse_errors, 'r-s', label='RMSE', markersize=4)
        plt.xlabel('Frame Index')
        plt.ylabel('Reconstruction Error')
        plt.title('Reconstruction Error vs Frame')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2: æœ€å¤§è¯¯å·®
        plt.subplot(2, 3, 2)
        plt.plot(frames, max_errors, 'g-^', label='Max Error', markersize=4)
        plt.xlabel('Frame Index')
        plt.ylabel('Max Error')
        plt.title('Maximum Error vs Frame')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾3: è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        plt.subplot(2, 3, 3)
        all_errors = []
        for frame_idx in frame_idx_list:
            if frame_idx in self.results['frame_results']:
                all_errors.extend(self.results['frame_results'][frame_idx]['vertex_errors'])
        
        plt.hist(all_errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        plt.xlabel('Vertex Error')
        plt.ylabel('Frequency')
        plt.title('Error Distribution (All Vertices)')
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾4: è¯¯å·®æ•£ç‚¹å›¾ï¼ˆå¸§vså¹³å‡è¯¯å·®ï¼‰
        plt.subplot(2, 3, 4)
        plt.scatter(frames, mean_errors, alpha=0.6, c=mean_errors, cmap='viridis')
        plt.colorbar(label='Mean Error')
        plt.xlabel('Frame Index')
        plt.ylabel('Mean Error')
        plt.title('Error Scatter Plot')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾5: æ€§èƒ½åˆ†æ
        plt.subplot(2, 3, 5)
        lbs_times = [self.results['frame_results'][f]['lbs_time'] for f in frames]
        plt.plot(frames, lbs_times, 'purple', marker='o', markersize=4)
        plt.xlabel('Frame Index')
        plt.ylabel('LBS Time (seconds)')
        plt.title('Performance vs Frame')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾6: è¯¯å·®ç®±å‹å›¾ï¼ˆæŒ‰å¸§åˆ†ç»„ï¼‰
        plt.subplot(2, 3, 6)
        if len(frames) <= 10:  # åªæœ‰å½“å¸§æ•°ä¸å¤ªå¤šæ—¶æ‰ç”»ç®±å‹å›¾
            error_data = []
            frame_labels = []
            for frame_idx in frames:
                if frame_idx in self.results['frame_results']:
                    error_data.append(self.results['frame_results'][frame_idx]['vertex_errors'])
                    frame_labels.append(str(frame_idx))
            
            plt.boxplot(error_data, labels=frame_labels)
            plt.xlabel('Frame Index')
            plt.ylabel('Vertex Error')
            plt.title('Error Distribution by Frame')
            plt.xticks(rotation=45)
        else:
            # å¤ªå¤šå¸§æ—¶ï¼Œæ˜¾ç¤ºè¯¯å·®ç»Ÿè®¡
            p25_errors = [np.percentile(self.results['frame_results'][f]['vertex_errors'], 25) for f in frames]
            p75_errors = [np.percentile(self.results['frame_results'][f]['vertex_errors'], 75) for f in frames]
            p95_errors = [np.percentile(self.results['frame_results'][f]['vertex_errors'], 95) for f in frames]
            
            plt.plot(frames, p25_errors, label='25th percentile', alpha=0.7)
            plt.plot(frames, p75_errors, label='75th percentile', alpha=0.7)
            plt.plot(frames, p95_errors, label='95th percentile', alpha=0.7)
            plt.xlabel('Frame Index')
            plt.ylabel('Error Percentiles')
            plt.title('Error Percentiles vs Frame')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / 'lbs_validation_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {output_path / 'lbs_validation_analysis.png'}")
        
        # 2. ç”Ÿæˆè¯¦ç»†çš„è¯¯å·®åˆ†æå›¾
        self._generate_detailed_error_analysis(output_path, frame_idx_list)
    
    def _generate_detailed_error_analysis(self, output_path, frame_idx_list):
        """ç”Ÿæˆè¯¦ç»†çš„è¯¯å·®åˆ†æå›¾"""
        plt.figure(figsize=(12, 8))
        
        # æ”¶é›†æ‰€æœ‰è¯¯å·®æ•°æ®
        all_frame_errors = {}
        for frame_idx in frame_idx_list:
            if frame_idx in self.results['frame_results']:
                all_frame_errors[frame_idx] = self.results['frame_results'][frame_idx]['vertex_errors']
        
        if not all_frame_errors:
            return
        
        # å­å›¾1: è¯¯å·®çƒ­åŠ›å›¾ï¼ˆå¦‚æœå¸§æ•°åˆé€‚ï¼‰
        plt.subplot(2, 2, 1)
        if len(all_frame_errors) <= 20:
            error_matrix = []
            frame_indices = sorted(all_frame_errors.keys())
            
            # è®¡ç®—æ¯å¸§çš„è¯¯å·®åˆ†ä½æ•°
            for frame_idx in frame_indices:
                errors = all_frame_errors[frame_idx]
                percentiles = [np.percentile(errors, p) for p in [10, 25, 50, 75, 90, 95, 99]]
                error_matrix.append(percentiles)
            
            error_matrix = np.array(error_matrix).T
            im = plt.imshow(error_matrix, aspect='auto', cmap='viridis', interpolation='nearest')
            plt.colorbar(im, label='Error Value')
            plt.yticks(range(len(['10%', '25%', '50%', '75%', '90%', '95%', '99%'])), 
                      ['10%', '25%', '50%', '75%', '90%', '95%', '99%'])
            plt.xticks(range(len(frame_indices)), [str(f) for f in frame_indices], rotation=45)
            plt.xlabel('Frame Index')
            plt.ylabel('Error Percentile')
            plt.title('Error Heatmap by Frame')
        else:
            # å¤ªå¤šå¸§æ—¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            plt.text(0.5, 0.5, f'Too many frames ({len(all_frame_errors)}) for heatmap\nUse frame range analysis instead', 
                    ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('Error Heatmap (Too Many Frames)')
        
        # å­å›¾2: ç´¯ç§¯è¯¯å·®åˆ†å¸ƒ
        plt.subplot(2, 2, 2)
        all_errors_combined = []
        for errors in all_frame_errors.values():
            all_errors_combined.extend(errors)
        
        sorted_errors = np.sort(all_errors_combined)
        cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)
        plt.plot(sorted_errors, cumulative)
        plt.xlabel('Error Value')
        plt.ylabel('Cumulative Probability')
        plt.title('Cumulative Error Distribution')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾3: è¯¯å·®ä¸è·ç¦»å‚è€ƒå¸§çš„å…³ç³»
        plt.subplot(2, 2, 3)
        distances_from_ref = []
        frame_mean_errors = []
        
        for frame_idx in sorted(all_frame_errors.keys()):
            distance = abs(frame_idx - self.reference_frame_idx)
            distances_from_ref.append(distance)
            frame_mean_errors.append(np.mean(all_frame_errors[frame_idx]))
        
        plt.scatter(distances_from_ref, frame_mean_errors, alpha=0.6)
        plt.xlabel('Distance from Reference Frame')
        plt.ylabel('Mean Reconstruction Error')
        plt.title('Error vs Distance from Reference')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(distances_from_ref) > 1:
            z = np.polyfit(distances_from_ref, frame_mean_errors, 1)
            p = np.poly1d(z)
            plt.plot(distances_from_ref, p(distances_from_ref), "r--", alpha=0.8, label=f'Trend: y={z[0]:.6f}x+{z[1]:.6f}')
            plt.legend()
        
        # å­å›¾4: è¯¯å·®æ–¹å·®åˆ†æ
        plt.subplot(2, 2, 4)
        frame_std_errors = [np.std(all_frame_errors[f]) for f in sorted(all_frame_errors.keys())]
        frame_indices = sorted(all_frame_errors.keys())
        
        plt.plot(frame_indices, frame_std_errors, 'o-', color='orange')
        plt.xlabel('Frame Index')
        plt.ylabel('Error Standard Deviation')
        plt.title('Error Variance by Frame')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / 'lbs_detailed_error_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: {output_path / 'lbs_detailed_error_analysis.png'}")
    
    def save_results(self, output_path="output/lbs_validation/validation_results.json"):
        """
        ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not self.results:
            print("æ²¡æœ‰å¯ä¿å­˜çš„ç»“æœ")
            return
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç»“æœ
        serializable_results = {
            'summary': self.results.get('summary', {}),
            'test_params': self.results.get('test_params', {}),
            'frame_results': {}
        }
        
        # è½¬æ¢å¸§ç»“æœï¼ˆç§»é™¤numpyæ•°ç»„ï¼‰
        for frame_idx, frame_result in self.results.get('frame_results', {}).items():
            serializable_frame_result = {
                'frame_idx': frame_result['frame_idx'],
                'mean_error': float(frame_result['mean_error']),
                'max_error': float(frame_result['max_error']),
                'min_error': float(frame_result['min_error']),
                'std_error': float(frame_result['std_error']),
                'median_error': float(frame_result['median_error']),
                'rmse': float(frame_result['rmse']),
                'p95_error': float(frame_result['p95_error']),
                'p99_error': float(frame_result['p99_error']),
                'lbs_time': float(frame_result['lbs_time']),
                'num_vertices': int(frame_result['num_vertices'])
            }
            serializable_results['frame_results'][str(frame_idx)] = serializable_frame_result
        
        # ä¿å­˜ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜: {output_path}")
    
    def export_mesh_comparison(self, frame_idx, output_dir="output/lbs_validation/meshes"):
        """
        å¯¼å‡ºç‰¹å®šå¸§çš„ç½‘æ ¼æ¯”è¾ƒï¼ˆåŸå§‹vsé‡å»ºï¼‰
        
        Args:
            frame_idx: å¸§ç´¢å¼•
            output_dir: è¾“å‡ºç›®å½•
        """
        if frame_idx not in self.results.get('frame_results', {}):
            print(f"å¸§ {frame_idx} æœªæµ‹è¯•ï¼Œè¯·å…ˆè¿è¡Œæµ‹è¯•")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        frame_result = self.results['frame_results'][frame_idx]
        
        # åˆ›å»ºåŸå§‹ç½‘æ ¼
        original_mesh = o3d.io.read_triangle_mesh(str(self.canonicalizer.mesh_files[frame_idx]))
        
        # åˆ›å»ºé‡å»ºç½‘æ ¼
        reconstructed_mesh = o3d.geometry.TriangleMesh()
        reconstructed_mesh.vertices = o3d.utility.Vector3dVector(frame_result['predicted_vertices'])
        if hasattr(original_mesh, 'triangles') and len(original_mesh.triangles) > 0:
            reconstructed_mesh.triangles = original_mesh.triangles
        
        # ä¿å­˜ç½‘æ ¼
        o3d.io.write_triangle_mesh(str(output_path / f"frame_{frame_idx:06d}_original.obj"), original_mesh)
        o3d.io.write_triangle_mesh(str(output_path / f"frame_{frame_idx:06d}_reconstructed.obj"), reconstructed_mesh)
        
        # åˆ›å»ºè¯¯å·®é¢œè‰²ç¼–ç çš„ç½‘æ ¼
        vertex_errors = frame_result['vertex_errors']
        error_colors = plt.cm.viridis(vertex_errors / np.max(vertex_errors))[:, :3]
        
        error_mesh = o3d.geometry.TriangleMesh()
        error_mesh.vertices = original_mesh.vertices
        error_mesh.triangles = original_mesh.triangles
        error_mesh.vertex_colors = o3d.utility.Vector3dVector(error_colors)
        
        o3d.io.write_triangle_mesh(str(output_path / f"frame_{frame_idx:06d}_error_colored.obj"), error_mesh)
        
        print(f"âœ… ç½‘æ ¼æ¯”è¾ƒå·²å¯¼å‡ºåˆ°: {output_path}")
        print(f"   - frame_{frame_idx:06d}_original.obj")
        print(f"   - frame_{frame_idx:06d}_reconstructed.obj") 
        print(f"   - frame_{frame_idx:06d}_error_colored.obj")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='LBSæƒé‡éªŒè¯æµ‹è¯•')
    parser.add_argument('--skeleton_dir', default='output/skeleton_prediction', 
                       help='éª¨éª¼æ•°æ®ç›®å½•')
    parser.add_argument('--mesh_dir', default='D:/Code/VVEditor/Rafa_Approves_hd_4k',
                       help='ç½‘æ ¼æ–‡ä»¶ç›®å½•')
    parser.add_argument('--weights_path', default='output/skinning_weights_fast.npz',
                       help='æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--reference_frame', type=int, default=5,
                       help='å‚è€ƒå¸§ç´¢å¼•')
    parser.add_argument('--start_frame', type=int, default=0,
                       help='æµ‹è¯•èµ·å§‹å¸§')
    parser.add_argument('--end_frame', type=int, default=None,
                       help='æµ‹è¯•ç»“æŸå¸§')
    parser.add_argument('--step', type=int, default=1,
                       help='æµ‹è¯•æ­¥é•¿')
    parser.add_argument('--output_dir', default='output/lbs_validation',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--export_meshes', nargs='*', type=int,
                       help='å¯¼å‡ºç‰¹å®šå¸§çš„ç½‘æ ¼æ¯”è¾ƒ')
    
    args = parser.parse_args()
    
    print("ğŸ” LBSæƒé‡éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–éªŒè¯å™¨
    try:
        validator = LBSValidator(
            skeleton_data_dir=args.skeleton_dir,
            mesh_folder_path=args.mesh_dir,
            weights_path=args.weights_path,
            reference_frame_idx=args.reference_frame
        )
        print(f"âœ… éªŒè¯å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æƒé‡æ–‡ä»¶: {args.weights_path}")
        print(f"   å‚è€ƒå¸§: {args.reference_frame}")
    except Exception as e:
        print(f"âŒ éªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # è¿è¡Œæµ‹è¯•
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•...")
    try:
        results = validator.test_frame_range(
            start_frame=args.start_frame,
            end_frame=args.end_frame,
            step=args.step,
            verbose=True
        )
        
        # åˆ†æè¯¯å·®åˆ†å¸ƒ
        validator.analyze_error_distribution()
        
        # ç”Ÿæˆå¯è§†åŒ–
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        validator.generate_visualization_plots(output_dir=args.output_dir)
        
        # ä¿å­˜ç»“æœ
        validator.save_results(f"{args.output_dir}/validation_results.json")
        
        # å¯¼å‡ºç‰¹å®šå¸§çš„ç½‘æ ¼
        if args.export_meshes is not None:
            print(f"\nğŸ’¾ å¯¼å‡ºç½‘æ ¼æ¯”è¾ƒ...")
            for frame_idx in args.export_meshes:
                if frame_idx in results['frame_results']:
                    validator.export_mesh_comparison(frame_idx, f"{args.output_dir}/meshes")
                else:
                    print(f"âš ï¸  å¸§ {frame_idx} æœªåœ¨æµ‹è¯•èŒƒå›´å†…")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print(f"\nğŸ‰ éªŒè¯æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}")


if __name__ == "__main__":
    main()
