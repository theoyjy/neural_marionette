import json
import numpy as np
import trimesh
import os
from pathlib import Path
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment, minimize
from sklearn.neighbors import NearestNeighbors
import pickle
from tqdm import tqdm
import open3d as o3d
from scipy.sparse import csr_matrix

class InverseMeshCanonicalizer:
    def __init__(self, skeleton_data_dir, reference_frame_idx=0):
        """
        åˆå§‹åŒ–åå‘ç½‘æ ¼ç»Ÿä¸€å™¨
        
        Args:
            skeleton_data_dir: åŒ…å«éª¨éª¼æ•°æ®npyæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
            reference_frame_idx: å‚è€ƒå¸§ç´¢å¼•ï¼ˆç”¨ä½œç»Ÿä¸€çš„ç›®æ ‡ï¼‰
        """
        self.skeleton_data_dir = Path(skeleton_data_dir)
        self.reference_frame_idx = reference_frame_idx
        
        # åŠ è½½éª¨éª¼æ•°æ®
        self.load_skeleton_data()
        
        self.reference_mesh = None
        self.canonicalization_maps = {}
        
        # æ¯å¸§çš„å½’ä¸€åŒ–å‚æ•°ï¼Œç”¨äºå¤„ç†æ¯ä¸ªmeshç‹¬ç«‹å½’ä¸€åŒ–çš„æƒ…å†µ
        self.frame_normalization_params = {}
        
        # LBSç›¸å…³å±æ€§
        self.skinning_weights = None  # [V, J] é¡¶ç‚¹åˆ°å…³èŠ‚çš„æƒé‡çŸ©é˜µ
        self.rest_pose_vertices = None  # é™æ¯å§¿æ€é¡¶ç‚¹åæ ‡
        self.rest_pose_transforms = None  # é™æ¯å§¿æ€å˜æ¢çŸ©é˜µ
        
    def load_skeleton_data(self):
        """åŠ è½½numpyæ ¼å¼çš„éª¨éª¼æ•°æ®"""
        try:
            # åŠ è½½å…³é”®ç‚¹æ•°æ® [num_frames, num_joints, 4] (x, y, z, confidence)
            self.keypoints = np.load(self.skeleton_data_dir / 'keypoints.npy')
            
            # åŠ è½½å˜æ¢çŸ©é˜µ [num_frames, num_joints, 4, 4]
            self.transforms = np.load(self.skeleton_data_dir / 'transforms.npy')
            
            # åŠ è½½çˆ¶èŠ‚ç‚¹å…³ç³» [num_joints]
            self.parents = np.load(self.skeleton_data_dir / 'parents.npy')
            
            self.num_frames, self.num_joints = self.keypoints.shape[0], self.keypoints.shape[1]
            
            print(f"æˆåŠŸåŠ è½½éª¨éª¼æ•°æ®:")
            print(f"  - å¸§æ•°: {self.num_frames}")
            print(f"  - å…³èŠ‚æ•°: {self.num_joints}")
            print(f"  - å…³é”®ç‚¹å½¢çŠ¶: {self.keypoints.shape} (åŒ…å«ç½®ä¿¡åº¦)")
            print(f"  - å˜æ¢çŸ©é˜µå½¢çŠ¶: {self.transforms.shape}")
            print(f"  - çˆ¶èŠ‚ç‚¹å…³ç³»å½¢çŠ¶: {self.parents.shape}")
            
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½éª¨éª¼æ•°æ®: {e}")
            
        # å°è¯•åŠ è½½å…¶ä»–å¯é€‰æ•°æ®
        try:
            if (self.skeleton_data_dir / 'affinity.npy').exists():
                self.affinity = np.load(self.skeleton_data_dir / 'affinity.npy')
                print(f"  - äº²å’Œåº¦çŸ©é˜µå½¢çŠ¶: {self.affinity.shape}")
            else:
                self.affinity = None
                
            if (self.skeleton_data_dir / 'priority.npy').exists():
                self.priority = np.load(self.skeleton_data_dir / 'priority.npy')
                print(f"  - ä¼˜å…ˆçº§å½¢çŠ¶: {self.priority.shape}")
            else:
                self.priority = None
                
            if (self.skeleton_data_dir / 'A.npy').exists():
                self.A = np.load(self.skeleton_data_dir / 'A.npy')
                print(f"  - AçŸ©é˜µå½¢çŠ¶: {self.A.shape}")
            else:
                self.A = None
                
            if (self.skeleton_data_dir / 'rotations.npy').exists():
                self.rotations = np.load(self.skeleton_data_dir / 'rotations.npy')
                print(f"  - æ—‹è½¬çŸ©é˜µå½¢çŠ¶: {self.rotations.shape}")
            else:
                self.rotations = None
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åŠ è½½å¯é€‰æ•°æ®: {e}")

    def compute_mesh_normalization_params(self, mesh):
        """
        è®¡ç®—å•ä¸ªmeshçš„å½’ä¸€åŒ–å‚æ•°ï¼ˆæ¨¡æ‹Ÿepisodic_normalizationçš„è¿‡ç¨‹ï¼‰
        
        Args:
            mesh: Open3D meshå¯¹è±¡
            
        Returns:
            normalization_params: å½’ä¸€åŒ–å‚æ•°å­—å…¸
        """
        vertices = np.asarray(mesh.vertices)
        
        # è®¡ç®—è¾¹ç•Œæ¡†ï¼ˆä¸episodic_normalizationç›¸åŒçš„é€»è¾‘ï¼‰
        bmax = np.amax(vertices, axis=0)
        bmin = np.amin(vertices, axis=0)
        blen = (bmax - bmin).max()
        
        # é»˜è®¤çš„å½’ä¸€åŒ–å‚æ•°ï¼ˆä¸episodic_normalizationé»˜è®¤å€¼ä¸€è‡´ï¼‰
        scale = 1.0
        x_trans = 0.0
        z_trans = 0.0
        
        params = {
            'bmin': bmin,
            'bmax': bmax,
            'blen': blen,
            'scale': scale,
            'x_trans': x_trans,
            'z_trans': z_trans
        }
        
        return params
    
    def normalize_mesh_vertices(self, vertices, normalization_params):
        """
        ä½¿ç”¨ç»™å®šçš„å½’ä¸€åŒ–å‚æ•°å°†meshé¡¶ç‚¹å½’ä¸€åŒ–
        
        Args:
            vertices: åŸå§‹é¡¶ç‚¹åæ ‡
            normalization_params: å½’ä¸€åŒ–å‚æ•°
            
        Returns:
            normalized_vertices: å½’ä¸€åŒ–åçš„é¡¶ç‚¹åæ ‡
        """
        params = normalization_params
        
        # åº”ç”¨ä¸episodic_normalizationç›¸åŒçš„å˜æ¢
        # å…¬å¼: ((seq - bmin) * scale / (blen + 1e-5)) * 2 - 1 + [x_trans, 0, z_trans]
        trans_offset = np.array([params['x_trans'], 0, params['z_trans']])
        normalized = ((vertices - params['bmin']) * params['scale'] / (params['blen'] + 1e-5)) * 2 - 1 + trans_offset
        
        return normalized

    def load_mesh_sequence(self, mesh_folder_path):
        """
        åŠ è½½ç½‘æ ¼åºåˆ—
        
        Args:
            mesh_folder_path: åŒ…å«objæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
        """
        self.mesh_folder_path = Path(mesh_folder_path)
        self.mesh_files = sorted(list(self.mesh_folder_path.glob("*.obj")))
        
        if len(self.mesh_files) != self.num_frames:
            print(f"è­¦å‘Š: ç½‘æ ¼æ–‡ä»¶æ•°é‡ ({len(self.mesh_files)}) ä¸éª¨éª¼å¸§æ•° ({self.num_frames}) ä¸åŒ¹é…")
        
        # åŠ è½½å‚è€ƒç½‘æ ¼
        self.reference_mesh = o3d.io.read_triangle_mesh(str(self.mesh_files[self.reference_frame_idx]))
        print(f"å‚è€ƒç½‘æ ¼é¡¶ç‚¹æ•°: {len(self.reference_mesh.vertices)}")
        
        # é¢„è®¡ç®—å‚è€ƒç½‘æ ¼çš„å½’ä¸€åŒ–å‚æ•°
        self.frame_normalization_params[self.reference_frame_idx] = self.compute_mesh_normalization_params(self.reference_mesh)
        print(f"å‚è€ƒç½‘æ ¼å½’ä¸€åŒ–å‚æ•°å·²è®¡ç®—")
        
        # é¢„è®¡ç®—å‚è€ƒç½‘æ ¼çš„å…³èŠ‚åˆ†é…ï¼ˆç”¨äºåŠ é€Ÿåç»­å¯¹åº”å…³ç³»è®¡ç®—ï¼‰
        ref_vertices = np.asarray(self.reference_mesh.vertices)
        ref_norm_vertices = self.normalize_mesh_vertices(ref_vertices, self.frame_normalization_params[self.reference_frame_idx])
        ref_joints = self.keypoints[self.reference_frame_idx, :, :3]
        self.reference_vertex_joints = self.assign_dominant_joints(ref_norm_vertices, ref_joints)
        print(f"å‚è€ƒç½‘æ ¼å…³èŠ‚åˆ†é…å·²é¢„è®¡ç®—")

    def compute_bone_influenced_vertices(self, mesh, frame_idx, influence_radius=0.1):
        """
        è®¡ç®—å—éª¨éª¼å½±å“çš„é¡¶ç‚¹
        
        Args:
            mesh
            frame_idx: å¸§ç´¢å¼•
            influence_radius: å½±å“åŠå¾„
            
        Returns:
            vertex_bone_weights: é¡¶ç‚¹åˆ°éª¨éª¼çš„æƒé‡çŸ©é˜µ [V, J]
        """
        # è·å–åŸå§‹é¡¶ç‚¹åæ ‡
        vertices = np.asarray(mesh.vertices)
        
        # è·å–å½’ä¸€åŒ–ç©ºé—´çš„keypointsï¼Œåªå–å‰3ä¸ªåæ ‡ï¼ˆå¿½ç•¥ç½®ä¿¡åº¦ï¼‰
        normalized_keypoints = self.keypoints[frame_idx, :, :3]  # [num_joints, 3]
        
        # è®¡ç®—å½“å‰meshçš„å½’ä¸€åŒ–å‚æ•°
        if frame_idx not in self.frame_normalization_params:
            self.frame_normalization_params[frame_idx] = self.compute_mesh_normalization_params(mesh)
        
        # å°†meshé¡¶ç‚¹å½’ä¸€åŒ–åˆ°ä¸keypointsç›¸åŒçš„ç©ºé—´
        normalized_vertices = self.normalize_mesh_vertices(vertices, self.frame_normalization_params[frame_idx])
        
        # åœ¨å½’ä¸€åŒ–ç©ºé—´ä¸­è®¡ç®—è·ç¦»
        distances = cdist(normalized_vertices, normalized_keypoints)
        
        # ä½¿ç”¨é«˜æ–¯æƒé‡
        weights = np.exp(-distances**2 / (2 * influence_radius**2))
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / (np.sum(weights, axis=1, keepdims=True) + 1e-8)
        
        return weights
    
    def compute_vertex_features(self, mesh, frame_idx):
        """
        è®¡ç®—é¡¶ç‚¹ç‰¹å¾ç”¨äºåŒ¹é…
        
        Args:
            mesh
            frame_idx: å¸§ç´¢å¼•
            
        Returns:
            features: é¡¶ç‚¹ç‰¹å¾çŸ©é˜µ [V, D]
        """
        vertices = np.asarray(mesh.vertices)
        
        # å‡ ä½•ç‰¹å¾
        geometric_features = []
        
        # 1. é¡¶ç‚¹åæ ‡ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
        # è®¡ç®—å½“å‰meshçš„å½’ä¸€åŒ–å‚æ•°ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¡ç®—è¿‡ï¼‰
        if frame_idx not in self.frame_normalization_params:
            self.frame_normalization_params[frame_idx] = self.compute_mesh_normalization_params(mesh)
        
        # å°†é¡¶ç‚¹å½’ä¸€åŒ–åˆ°ä¸keypointsç›¸åŒçš„ç©ºé—´
        normalized_vertices = self.normalize_mesh_vertices(vertices, self.frame_normalization_params[frame_idx])
        geometric_features.append(normalized_vertices)
        
        # 2. éª¨éª¼ç©ºé—´å˜æ¢çš„é¡¶ç‚¹åæ ‡ï¼ˆå¯é€‰ï¼Œä½œä¸ºé¢å¤–ç‰¹å¾ï¼‰
        keypoints = self.keypoints[frame_idx, :, :3]  # [num_joints, 3] åªå–åæ ‡ï¼Œå¿½ç•¥ç½®ä¿¡åº¦
        transforms = self.transforms[frame_idx]  # [num_joints, 4, 4]
        
        # å°†å½’ä¸€åŒ–é¡¶ç‚¹å˜æ¢åˆ°éª¨éª¼ç©ºé—´
        vertices_homogeneous = np.hstack([normalized_vertices, np.ones((len(normalized_vertices), 1))])
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³èŠ‚çš„é€†å˜æ¢ä½œä¸ºæ ¹å˜æ¢
        if len(transforms) > 0:
            root_inv_transform = np.linalg.inv(transforms[0])
            canonical_vertices = (root_inv_transform @ vertices_homogeneous.T).T[:, :3]
        else:
            canonical_vertices = normalized_vertices
        
        geometric_features.append(canonical_vertices)
        
        # 3. æ³•å‘é‡
        if hasattr(mesh, 'vertex_normals') and mesh.vertex_normals is not None:
            geometric_features.append(np.asarray(mesh.vertex_normals))
        else:
            # è®¡ç®—æ³•å‘é‡
            mesh.compute_vertex_normals()
            geometric_features.append(np.asarray(mesh.vertex_normals))
        
        # 4. éª¨éª¼æƒé‡ç‰¹å¾
        bone_weights = self.compute_bone_influenced_vertices(mesh, frame_idx)
        geometric_features.append(bone_weights)
        
        # 5. æ›²ç‡ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼Œä¼˜åŒ–æ€§èƒ½ï¼‰
        try:
            # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•è®¡ç®—æ›²ç‡ç‰¹å¾
            # æ–¹æ³•1ï¼šåŸºäºé¡¶ç‚¹çš„å±€éƒ¨å¯†åº¦è€Œä¸æ˜¯æœ€è¿‘é‚»
            center = np.mean(vertices, axis=0)
            distances_to_center = np.linalg.norm(vertices - center, axis=1)
            
            # è®¡ç®—æ¯ä¸ªé¡¶ç‚¹åœ¨å±€éƒ¨åŒºåŸŸçš„å¯†åº¦ç‰¹å¾
            vertex_curvature = []
            sample_size = min(len(vertices), 1000)  # é™åˆ¶æ ·æœ¬å¤§å°ä»¥æé«˜æ€§èƒ½
            
            if len(vertices) > sample_size:
                # å¦‚æœé¡¶ç‚¹å¤ªå¤šï¼Œä½¿ç”¨é‡‡æ ·
                sample_indices = np.random.choice(len(vertices), sample_size, replace=False)
                sample_vertices = vertices[sample_indices]
            else:
                sample_vertices = vertices
                sample_indices = np.arange(len(vertices))
            
            # ä½¿ç”¨KDTreeè¿›è¡Œé«˜æ•ˆçš„æœ€è¿‘é‚»æœç´¢
            from sklearn.neighbors import NearestNeighbors
            nbrs = NearestNeighbors(n_neighbors=min(11, len(sample_vertices)), algorithm='kd_tree').fit(sample_vertices)
            
            for i in range(len(vertices)):
                vertex = vertices[i]
                
                # æ‰¾åˆ°æœ€è¿‘çš„é‚»å±…
                distances, indices = nbrs.kneighbors([vertex])
                neighbor_distances = distances[0][1:]  # æ’é™¤è‡ªå·±
                
                if len(neighbor_distances) > 0:
                    mean_dist = np.mean(neighbor_distances)
                    var_dist = np.var(neighbor_distances) if len(neighbor_distances) > 1 else 0.0
                else:
                    mean_dist = 0.0
                    var_dist = 0.0
                
                vertex_curvature.append([mean_dist, var_dist])
            
            geometric_features.append(np.array(vertex_curvature))
        except Exception as e:
            print(f"æ›²ç‡è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é›¶ç‰¹å¾: {e}")
            # å¦‚æœæ›²ç‡è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é›¶ç‰¹å¾
            geometric_features.append(np.zeros((len(vertices), 2)))
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        features = np.hstack(geometric_features)
        
        return features
    
    def compute_skeleton_driven_correspondence(self, target_mesh, target_frame_idx):
        """
        åŸºäºéª¨éª¼å…³èŠ‚é©±åŠ¨çš„å¿«é€Ÿé¡¶ç‚¹å¯¹åº”å…³ç³»è®¡ç®—
        
        Args:
            target_mesh: ç›®æ ‡ç½‘æ ¼
            target_frame_idx: ç›®æ ‡å¸§ç´¢å¼•
            
        Returns:
            correspondence_map: ä»ç›®æ ‡é¡¶ç‚¹ç´¢å¼•åˆ°å‚è€ƒé¡¶ç‚¹ç´¢å¼•çš„æ˜ å°„
        """
        # è·å–å‚è€ƒå¸§å’Œç›®æ ‡å¸§çš„å…³èŠ‚ç‚¹
        ref_joints = self.keypoints[self.reference_frame_idx, :, :3]  # [num_joints, 3]
        target_joints = self.keypoints[target_frame_idx, :, :3]  # [num_joints, 3]
        
        # è·å–ç½‘æ ¼é¡¶ç‚¹
        ref_vertices = np.asarray(self.reference_mesh.vertices)
        target_vertices = np.asarray(target_mesh.vertices)
        
        # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
        if target_frame_idx not in self.frame_normalization_params:
            self.frame_normalization_params[target_frame_idx] = self.compute_mesh_normalization_params(target_mesh)
        
        # å½’ä¸€åŒ–é¡¶ç‚¹åˆ°ä¸keypointsç›¸åŒçš„ç©ºé—´
        ref_norm_vertices = self.normalize_mesh_vertices(ref_vertices, self.frame_normalization_params[self.reference_frame_idx])
        target_norm_vertices = self.normalize_mesh_vertices(target_vertices, self.frame_normalization_params[target_frame_idx])
        
        # ä½¿ç”¨é¢„è®¡ç®—çš„å‚è€ƒç½‘æ ¼å…³èŠ‚åˆ†é…
        ref_vertex_joints = self.reference_vertex_joints
        
        # ä¸ºç›®æ ‡ç½‘æ ¼é¡¶ç‚¹åˆ†é…ä¸»å¯¼å…³èŠ‚
        target_vertex_joints = self.assign_dominant_joints(target_norm_vertices, target_joints)
        
        # åŸºäºå…³èŠ‚å¯¹åº”å…³ç³»è¿›è¡Œé¡¶ç‚¹åŒ¹é…
        correspondence_map = self.match_vertices_by_skeleton(
            ref_norm_vertices, target_norm_vertices, 
            ref_vertex_joints, target_vertex_joints,
            ref_joints, target_joints
        )
        
        return correspondence_map
    
    def assign_dominant_joints(self, vertices, joints, top_k=3):
        """
        ä¸ºæ¯ä¸ªé¡¶ç‚¹åˆ†é…å½±å“æœ€å¤§çš„å‰kä¸ªå…³èŠ‚
        
        Args:
            vertices: é¡¶ç‚¹åæ ‡ [V, 3]
            joints: å…³èŠ‚ç‚¹åæ ‡ [J, 3] 
            top_k: ä¿ç•™å‰kä¸ªæœ€è¿‘å…³èŠ‚
            
        Returns:
            vertex_joints: æ¯ä¸ªé¡¶ç‚¹çš„ä¸»å¯¼å…³èŠ‚ä¿¡æ¯ [V, top_k, 2] (joint_idx, weight)
        """
        # è®¡ç®—é¡¶ç‚¹åˆ°å…³èŠ‚çš„è·ç¦»
        distances = cdist(vertices, joints)  # [V, J]
        
        # ä½¿ç”¨é«˜æ–¯æƒé‡
        sigma = 0.1
        weights = np.exp(-distances**2 / (2 * sigma**2))
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / (np.sum(weights, axis=1, keepdims=True) + 1e-8)
        
        # è·å–æ¯ä¸ªé¡¶ç‚¹çš„å‰kä¸ªæœ€å¼ºå…³èŠ‚
        vertex_joints = []
        for v in range(len(vertices)):
            top_indices = np.argsort(weights[v])[-top_k:][::-1]  # é™åº
            top_weights = weights[v][top_indices]
            vertex_joints.append(list(zip(top_indices, top_weights)))
        
        return vertex_joints
    
    def match_vertices_by_skeleton(self, ref_vertices, target_vertices, 
                                 ref_vertex_joints, target_vertex_joints,
                                 ref_joints, target_joints):
        """
        åŸºäºéª¨éª¼å…³èŠ‚å¯¹åº”å…³ç³»åŒ¹é…é¡¶ç‚¹
        
        Args:
            ref_vertices: å‚è€ƒé¡¶ç‚¹ [V_ref, 3]
            target_vertices: ç›®æ ‡é¡¶ç‚¹ [V_target, 3]
            ref_vertex_joints: å‚è€ƒé¡¶ç‚¹çš„å…³èŠ‚åˆ†é…
            target_vertex_joints: ç›®æ ‡é¡¶ç‚¹çš„å…³èŠ‚åˆ†é…
            ref_joints: å‚è€ƒå…³èŠ‚ç‚¹ [J, 3]
            target_joints: ç›®æ ‡å…³èŠ‚ç‚¹ [J, 3]
            
        Returns:
            correspondence_map: [V_target] -> V_ref
        """
        correspondence_map = np.zeros(len(target_vertices), dtype=int)
        
        # æŒ‰å…³èŠ‚åˆ†ç»„å¤„ç†é¡¶ç‚¹
        joint_groups = {}
        
        # å°†ç›®æ ‡é¡¶ç‚¹æŒ‰ä¸»å¯¼å…³èŠ‚åˆ†ç»„
        for v_idx, joint_list in enumerate(target_vertex_joints):
            primary_joint = joint_list[0][0]  # æœ€å¼ºå…³èŠ‚
            if primary_joint not in joint_groups:
                joint_groups[primary_joint] = []
            joint_groups[primary_joint].append(v_idx)
        
        print(f"æŒ‰{len(joint_groups)}ä¸ªå…³èŠ‚åˆ†ç»„å¤„ç†é¡¶ç‚¹...")
        
        for joint_idx, target_vertex_indices in joint_groups.items():
            if len(target_vertex_indices) == 0:
                continue
                
            # æ‰¾åˆ°å‚è€ƒç½‘æ ¼ä¸­å±äºåŒä¸€å…³èŠ‚çš„é¡¶ç‚¹
            ref_vertex_indices = []
            for v_idx, joint_list in enumerate(ref_vertex_joints):
                if len(joint_list) > 0 and joint_list[0][0] == joint_idx:
                    ref_vertex_indices.append(v_idx)
            
            if len(ref_vertex_indices) == 0:
                # å¦‚æœå‚è€ƒç½‘æ ¼ä¸­æ²¡æœ‰å¯¹åº”å…³èŠ‚çš„é¡¶ç‚¹ï¼Œä½¿ç”¨å…¨å±€æœ€è¿‘é‚»
                ref_vertex_indices = list(range(len(ref_vertices)))
            
            # åœ¨è¯¥å…³èŠ‚çš„å±€éƒ¨ç©ºé—´ä¸­è¿›è¡ŒåŒ¹é…
            target_local_vertices = target_vertices[target_vertex_indices]
            ref_local_vertices = ref_vertices[ref_vertex_indices]
            
            # è½¬æ¢åˆ°å…³èŠ‚å±€éƒ¨åæ ‡ç³»
            joint_center_target = target_joints[joint_idx]
            joint_center_ref = ref_joints[joint_idx]
            
            target_local_relative = target_local_vertices - joint_center_target
            ref_local_relative = ref_local_vertices - joint_center_ref
            
            # ä½¿ç”¨æœ€è¿‘é‚»åŒ¹é…
            if len(ref_local_vertices) > 0:
                nbrs = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(ref_local_relative)
                _, indices = nbrs.kneighbors(target_local_relative)
                
                # æ˜ å°„å›å…¨å±€ç´¢å¼•
                for i, target_v_idx in enumerate(target_vertex_indices):
                    ref_v_idx = ref_vertex_indices[indices[i][0]]
                    correspondence_map[target_v_idx] = ref_v_idx
        
        return correspondence_map
    
    def apply_lbs_transform(self, rest_vertices, weights, transforms):
        """
        åº”ç”¨Linear Blend Skinningå˜æ¢
        
        Args:
            rest_vertices: é™æ¯å§¿æ€é¡¶ç‚¹ [V, 3]
            weights: skinningæƒé‡ [V, J]
            transforms: å…³èŠ‚å˜æ¢çŸ©é˜µ [J, 4, 4]
            
        Returns:
            transformed_vertices: å˜æ¢åçš„é¡¶ç‚¹ [V, 3]
        """
        num_vertices = rest_vertices.shape[0]
        num_joints = transforms.shape[0]
        
        # å°†é¡¶ç‚¹è½¬æ¢ä¸ºé½æ¬¡åæ ‡
        rest_vertices_homo = np.hstack([rest_vertices, np.ones((num_vertices, 1))])  # [V, 4]
        
        # åˆå§‹åŒ–è¾“å‡ºé¡¶ç‚¹
        transformed_vertices = np.zeros((num_vertices, 3))
        
        # å¯¹æ¯ä¸ªå…³èŠ‚åº”ç”¨å˜æ¢å¹¶æ··åˆ
        for j in range(num_joints):
            # è·å–å½“å‰å…³èŠ‚çš„å˜æ¢çŸ©é˜µ [4, 4]
            joint_transform = transforms[j]
            
            # å˜æ¢æ‰€æœ‰é¡¶ç‚¹
            transformed_homo = (joint_transform @ rest_vertices_homo.T).T  # [V, 4]
            transformed_xyz = transformed_homo[:, :3]  # [V, 3]
            
            # æ ¹æ®æƒé‡æ··åˆ
            joint_weights = weights[:, j:j+1]  # [V, 1]
            transformed_vertices += joint_weights * transformed_xyz
        
        return transformed_vertices
    
    def compute_lbs_loss(self, weights_flat, rest_vertices, target_vertices, transforms, 
                        regularization_lambda=0.01):
        """
        è®¡ç®—LBSæŸå¤±å‡½æ•°
        
        Args:
            weights_flat: å±•å¹³çš„æƒé‡å‘é‡ [V*J]
            rest_vertices: é™æ¯å§¿æ€é¡¶ç‚¹ [V, 3]
            target_vertices: ç›®æ ‡é¡¶ç‚¹ [V, 3]
            transforms: å…³èŠ‚å˜æ¢çŸ©é˜µ [J, 4, 4]
            regularization_lambda: æ­£åˆ™åŒ–ç³»æ•°
            
        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        num_vertices = rest_vertices.shape[0]
        num_joints = transforms.shape[0]
        
        # é‡å¡‘æƒé‡çŸ©é˜µ
        weights = weights_flat.reshape(num_vertices, num_joints)
        
        # ç¡®ä¿æƒé‡éè´Ÿä¸”å½’ä¸€åŒ–
        weights = np.maximum(weights, 0)
        weights = weights / (np.sum(weights, axis=1, keepdims=True) + 1e-8)
        
        # åº”ç”¨LBSå˜æ¢
        predicted_vertices = self.apply_lbs_transform(rest_vertices, weights, transforms)
        
        # è®¡ç®—é‡å»ºæŸå¤±
        reconstruction_loss = np.mean(np.sum((predicted_vertices - target_vertices)**2, axis=1))
        
        # æ·»åŠ ç¨€ç–æ€§æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¯ä¸ªé¡¶ç‚¹åªå—å°‘æ•°å…³èŠ‚å½±å“ï¼‰
        sparsity_loss = np.mean(np.sum(weights**2, axis=1))
        
        # æ·»åŠ å¹³æ»‘æ€§æ­£åˆ™åŒ–ï¼ˆå¯é€‰ï¼Œéœ€è¦ç½‘æ ¼è¿æ¥ä¿¡æ¯ï¼‰
        smoothness_loss = 0.0
        
        total_loss = reconstruction_loss + regularization_lambda * sparsity_loss + smoothness_loss
        
        return total_loss
    
    def optimize_skinning_weights_for_frame(self, target_frame_idx, max_iter=1000, 
                                          init_method='distance_based', regularization_lambda=0.01):
        """
        ä¸ºç‰¹å®šå¸§ä¼˜åŒ–skinningæƒé‡
        
        Args:
            target_frame_idx: ç›®æ ‡å¸§ç´¢å¼•
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            init_method: åˆå§‹åŒ–æ–¹æ³• ('distance_based', 'uniform', 'random')
            
        Returns:
            optimized_weights: ä¼˜åŒ–åçš„æƒé‡çŸ©é˜µ [V, J]
            loss_history: æŸå¤±å†å²
        """
        # è·å–æ•°æ®
        rest_vertices = self.rest_pose_vertices  # ä½¿ç”¨reference frameä½œä¸ºrest pose
        target_mesh = o3d.io.read_triangle_mesh(str(self.mesh_files[target_frame_idx]))
        target_vertices = np.asarray(target_mesh.vertices)
        
        # å½’ä¸€åŒ–å¤„ç†ï¼ˆä¿æŒä¸keypointsç›¸åŒçš„ç©ºé—´ï¼‰
        if target_frame_idx not in self.frame_normalization_params:
            self.frame_normalization_params[target_frame_idx] = self.compute_mesh_normalization_params(target_mesh)
        
        target_vertices_norm = self.normalize_mesh_vertices(target_vertices, self.frame_normalization_params[target_frame_idx])
        rest_vertices_norm = self.normalize_mesh_vertices(rest_vertices, self.frame_normalization_params[self.reference_frame_idx])
        
        # è·å–å˜æ¢çŸ©é˜µ
        target_transforms = self.transforms[target_frame_idx]  # [J, 4, 4]
        rest_transforms = self.transforms[self.reference_frame_idx]  # [J, 4, 4]
        
        # è®¡ç®—ç›¸å¯¹å˜æ¢ï¼ˆä»rest poseåˆ°target poseï¼‰
        relative_transforms = np.zeros_like(target_transforms)
        for j in range(self.num_joints):
            if np.linalg.det(rest_transforms[j][:3, :3]) > 1e-6:  # æ£€æŸ¥æ˜¯å¦å¯é€†
                rest_inv = np.linalg.inv(rest_transforms[j])
                relative_transforms[j] = target_transforms[j] @ rest_inv
            else:
                relative_transforms[j] = np.eye(4)
        
        num_vertices = len(rest_vertices_norm)
        num_joints = self.num_joints
        
        # åˆå§‹åŒ–æƒé‡
        if init_method == 'distance_based':
            # åŸºäºè·ç¦»çš„åˆå§‹åŒ–
            keypoints = self.keypoints[self.reference_frame_idx, :, :3]
            distances = cdist(rest_vertices_norm, keypoints)
            weights_init = np.exp(-distances**2 / (2 * 0.1**2))
            weights_init = weights_init / (np.sum(weights_init, axis=1, keepdims=True) + 1e-8)
        elif init_method == 'uniform':
            # å‡åŒ€åˆå§‹åŒ–
            weights_init = np.ones((num_vertices, num_joints)) / num_joints
        else:
            # éšæœºåˆå§‹åŒ–
            weights_init = np.random.rand(num_vertices, num_joints)
            weights_init = weights_init / (np.sum(weights_init, axis=1, keepdims=True) + 1e-8)
        
        # å±•å¹³æƒé‡ç”¨äºä¼˜åŒ–
        weights_flat_init = weights_init.flatten()
        
        # å®šä¹‰ç›®æ ‡å‡½æ•°
        def objective(weights_flat):
            return self.compute_lbs_loss(weights_flat, rest_vertices_norm, target_vertices_norm, 
                                       relative_transforms, regularization_lambda)
        
        # ä½¿ç”¨é«˜æ•ˆçš„ä¼˜åŒ–æ–¹æ³•ï¼šå¤§å—å¹¶è¡Œä¼˜åŒ–
        print(f"ä½¿ç”¨é«˜æ•ˆä¼˜åŒ–æ–¹æ³•...")
        print(f"é¡¶ç‚¹æ•°: {num_vertices}, å…³èŠ‚æ•°: {num_joints}")
        
        # å¤§å¹…å‡å°‘è®¡ç®—é‡
        if num_vertices > 10000:
            # å¯¹äºå¤§ç½‘æ ¼ï¼Œä½¿ç”¨é‡‡æ ·ç­–ç•¥
            sample_size = 5000  # åªä¼˜åŒ–5000ä¸ªä»£è¡¨æ€§é¡¶ç‚¹
            sample_indices = np.random.choice(num_vertices, sample_size, replace=False)
            print(f"å¤§ç½‘æ ¼æ£€æµ‹ï¼Œé‡‡æ · {sample_size} ä¸ªé¡¶ç‚¹è¿›è¡Œä¼˜åŒ–")
            
            # é‡‡æ ·é¡¶ç‚¹å’Œç›®æ ‡
            sampled_rest = rest_vertices_norm[sample_indices]
            sampled_target = target_vertices_norm[sample_indices]
            sampled_weights_init = weights_init[sample_indices]
            
            # ä¼˜åŒ–é‡‡æ ·çš„æƒé‡
            optimized_sampled_weights = self.optimize_sampled_weights(
                sampled_rest, sampled_target, sampled_weights_init, 
                relative_transforms, regularization_lambda, max_iter // 5
            )
            
            # å°†ä¼˜åŒ–ç»“æœæ’å€¼åˆ°æ‰€æœ‰é¡¶ç‚¹
            optimized_weights = weights_init.copy()
            optimized_weights[sample_indices] = optimized_sampled_weights
            
            # å¯¹æœªé‡‡æ ·çš„é¡¶ç‚¹ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
            from sklearn.neighbors import NearestNeighbors
            nbrs = NearestNeighbors(n_neighbors=3, algorithm='kd_tree').fit(sampled_rest)
            distances, indices = nbrs.kneighbors(rest_vertices_norm)
            
            for i in range(num_vertices):
                if i not in sample_indices:
                    # ä½¿ç”¨è·ç¦»åŠ æƒå¹³å‡
                    weights_sum = np.sum(1.0 / (distances[i] + 1e-6))
                    weighted_weights = np.zeros(num_joints)
                    for j, neighbor_idx in enumerate(indices[i]):
                        weight = (1.0 / (distances[i][j] + 1e-6)) / weights_sum
                        weighted_weights += weight * optimized_sampled_weights[neighbor_idx]
                    optimized_weights[i] = weighted_weights
                    # é‡æ–°å½’ä¸€åŒ–
                    optimized_weights[i] = optimized_weights[i] / (np.sum(optimized_weights[i]) + 1e-8)
        else:
            # å¯¹äºå°ç½‘æ ¼ï¼Œä½¿ç”¨æ ‡å‡†ä¼˜åŒ–
            optimized_weights = self.optimize_standard_weights(
                rest_vertices_norm, target_vertices_norm, weights_init,
                relative_transforms, regularization_lambda, max_iter // 5
            )
        
        # è®¡ç®—æœ€ç»ˆæŸå¤±
        final_loss = self.compute_lbs_loss(optimized_weights.flatten(), rest_vertices_norm, 
                                         target_vertices_norm, relative_transforms, regularization_lambda)
        
        print(f"ä¼˜åŒ–å®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {final_loss:.6f}")
        
        return optimized_weights, [final_loss]
    
    def optimize_sampled_weights(self, rest_vertices, target_vertices, weights_init, 
                               relative_transforms, regularization_lambda, max_iter):
        """
        ä¼˜åŒ–é‡‡æ ·çš„æƒé‡ï¼ˆè¾ƒå°è§„æ¨¡ï¼‰
        """
        num_vertices, num_joints = weights_init.shape
        optimized_weights = weights_init.copy()
        
        print(f"ä¼˜åŒ–é‡‡æ ·æƒé‡: {num_vertices} é¡¶ç‚¹")
        
        # ä½¿ç”¨å—ä¼˜åŒ–ï¼Œä½†å—æ›´å¤§
        chunk_size = 500  # æ›´å¤§çš„å—
        learning_rate = 0.02  # æ›´å¤§çš„å­¦ä¹ ç‡
        
        for iteration in range(max_iter):
            total_loss = 0.0
            
            # éšæœºæ‰“ä¹±é¡¶ç‚¹é¡ºåº
            perm = np.random.permutation(num_vertices)
            
            for start_idx in range(0, num_vertices, chunk_size):
                end_idx = min(start_idx + chunk_size, num_vertices)
                chunk_indices = perm[start_idx:end_idx]
                
                # æå–å½“å‰å—
                chunk_rest = rest_vertices[chunk_indices]
                chunk_target = target_vertices[chunk_indices]
                chunk_weights = optimized_weights[chunk_indices].copy()
                
                # ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™ - åªåšä¸€æ¬¡å†…å±‚è¿­ä»£
                predicted = self.apply_lbs_transform(chunk_rest, chunk_weights, relative_transforms)
                error = predicted - chunk_target
                
                # ç®€åŒ–çš„æ¢¯åº¦è®¡ç®— - åªè®¡ç®—ä¸»è¦å…³èŠ‚çš„æ¢¯åº¦
                gradient = np.zeros_like(chunk_weights)
                eps = 1e-5
                
                for i in range(len(chunk_weights)):
                    # åªä¼˜åŒ–æƒé‡æœ€å¤§çš„å‰5ä¸ªå…³èŠ‚
                    top_joints = np.argsort(chunk_weights[i])[-5:]
                    
                    for j in top_joints:
                        chunk_weights_plus = chunk_weights.copy()
                        chunk_weights_plus[i, j] += eps
                        chunk_weights_plus[i] = chunk_weights_plus[i] / (np.sum(chunk_weights_plus[i]) + 1e-8)
                        
                        predicted_plus = self.apply_lbs_transform(chunk_rest, chunk_weights_plus, relative_transforms)
                        error_plus = predicted_plus - chunk_target
                        
                        loss = np.mean(np.sum(error**2, axis=1))
                        loss_plus = np.mean(np.sum(error_plus**2, axis=1))
                        
                        gradient[i, j] = (loss_plus - loss) / eps
                
                # æ›´æ–°æƒé‡
                chunk_weights -= learning_rate * gradient
                chunk_weights = np.maximum(chunk_weights, 0)
                chunk_weights = chunk_weights / (np.sum(chunk_weights, axis=1, keepdims=True) + 1e-8)
                
                optimized_weights[chunk_indices] = chunk_weights
                
                # è®¡ç®—æŸå¤±
                predicted = self.apply_lbs_transform(chunk_rest, chunk_weights, relative_transforms)
                chunk_loss = np.mean(np.sum((predicted - chunk_target)**2, axis=1))
                total_loss += chunk_loss * len(chunk_weights) / num_vertices
            
            if iteration % 10 == 0:
                print(f"  é‡‡æ ·ä¼˜åŒ–è¿­ä»£ {iteration}: æŸå¤± = {total_loss:.6f}")
        
        return optimized_weights
    
    def optimize_standard_weights(self, rest_vertices, target_vertices, weights_init,
                                relative_transforms, regularization_lambda, max_iter):
        """
        æ ‡å‡†æƒé‡ä¼˜åŒ–ï¼ˆä¸­ç­‰è§„æ¨¡ï¼‰
        """
        num_vertices, num_joints = weights_init.shape
        optimized_weights = weights_init.copy()
        
        print(f"æ ‡å‡†ä¼˜åŒ–: {num_vertices} é¡¶ç‚¹")
        
        chunk_size = 200  # é€‚ä¸­çš„å—å¤§å°
        learning_rate = 0.01
        
        for iteration in range(max_iter):
            total_loss = 0.0
            
            for start_idx in range(0, num_vertices, chunk_size):
                end_idx = min(start_idx + chunk_size, num_vertices)
                
                chunk_rest = rest_vertices[start_idx:end_idx]
                chunk_target = target_vertices[start_idx:end_idx]
                chunk_weights = optimized_weights[start_idx:end_idx].copy()
                
                # ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™
                for sub_iter in range(2):  # åªåš2æ¬¡å†…å±‚è¿­ä»£
                    predicted = self.apply_lbs_transform(chunk_rest, chunk_weights, relative_transforms)
                    error = predicted - chunk_target
                    
                    # è®¡ç®—æ¢¯åº¦ï¼ˆåªå¯¹éƒ¨åˆ†å…³èŠ‚ï¼‰
                    gradient = np.zeros_like(chunk_weights)
                    eps = 1e-5
                    
                    for i in range(min(len(chunk_weights), 50)):  # åªä¼˜åŒ–å‰50ä¸ªé¡¶ç‚¹
                        top_joints = np.argsort(chunk_weights[i])[-3:]  # åªä¼˜åŒ–å‰3ä¸ªå…³èŠ‚
                        
                        for j in top_joints:
                            chunk_weights_plus = chunk_weights.copy()
                            chunk_weights_plus[i, j] += eps
                            chunk_weights_plus[i] = chunk_weights_plus[i] / (np.sum(chunk_weights_plus[i]) + 1e-8)
                            
                            predicted_plus = self.apply_lbs_transform(chunk_rest, chunk_weights_plus, relative_transforms)
                            error_plus = predicted_plus - chunk_target
                            
                            loss = np.mean(np.sum(error**2, axis=1))
                            loss_plus = np.mean(np.sum(error_plus**2, axis=1))
                            
                            gradient[i, j] = (loss_plus - loss) / eps
                    
                    chunk_weights -= learning_rate * gradient
                    chunk_weights = np.maximum(chunk_weights, 0)
                    chunk_weights = chunk_weights / (np.sum(chunk_weights, axis=1, keepdims=True) + 1e-8)
                
                optimized_weights[start_idx:end_idx] = chunk_weights
                
                predicted = self.apply_lbs_transform(chunk_rest, chunk_weights, relative_transforms)
                chunk_loss = np.mean(np.sum((predicted - chunk_target)**2, axis=1))
                total_loss += chunk_loss * len(chunk_weights) / num_vertices
            
            if iteration % 5 == 0:
                print(f"  æ ‡å‡†ä¼˜åŒ–è¿­ä»£ {iteration}: æŸå¤± = {total_loss:.6f}")
        
        return optimized_weights
    
    def optimize_reference_frame_skinning(self, regularization_lambda=0.01, max_iter=1000):
        """
        ä¼˜åŒ–reference frameçš„skinningæƒé‡
        
        Args:
            regularization_lambda: æ­£åˆ™åŒ–ç³»æ•°
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            skinning_weights: ä¼˜åŒ–åçš„æƒé‡çŸ©é˜µ [V, J]
        """
        # è®¾ç½®rest poseä¸ºreference frame
        self.rest_pose_vertices = np.asarray(self.reference_mesh.vertices)
        self.rest_pose_transforms = self.transforms[self.reference_frame_idx]
        
        print(f"å¼€å§‹ä¼˜åŒ–reference frame (frame {self.reference_frame_idx}) çš„skinningæƒé‡...")
        
        # å¯¹æ‰€æœ‰å…¶ä»–å¸§è¿›è¡Œä¼˜åŒ–
        all_weights = []
        all_losses = []
        
        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§å¸§è¿›è¡Œä¼˜åŒ–
        optimization_frames = []
        total_frames = len(self.mesh_files)
        
        # é€‰æ‹©ç­–ç•¥ï¼šå‡åŒ€é‡‡æ · + åŒ…å«é¦–å°¾å¸§
        if total_frames <= 10:
            optimization_frames = list(range(total_frames))
        else:
            # å‡åŒ€é‡‡æ ·10å¸§
            step = total_frames // 10
            optimization_frames = list(range(0, total_frames, step))
            if optimization_frames[-1] != total_frames - 1:
                optimization_frames.append(total_frames - 1)
        
        # ç§»é™¤reference frame
        if self.reference_frame_idx in optimization_frames:
            optimization_frames.remove(self.reference_frame_idx)
        
        print(f"å°†ä½¿ç”¨ {len(optimization_frames)} å¸§è¿›è¡Œæƒé‡ä¼˜åŒ–: {optimization_frames}")
        
        # ä¸ºæ¯ä¸€å¸§ä¼˜åŒ–æƒé‡
        for frame_idx in tqdm(optimization_frames, desc="ä¼˜åŒ–å„å¸§æƒé‡"):
            weights, loss_history = self.optimize_skinning_weights_for_frame(
                frame_idx, max_iter=max_iter, regularization_lambda=regularization_lambda
            )
            all_weights.append(weights)
            all_losses.extend(loss_history)
        
        # å¹³å‡æ‰€æœ‰å¸§çš„æƒé‡ä½œä¸ºæœ€ç»ˆç»“æœ
        if all_weights:
            self.skinning_weights = np.mean(all_weights, axis=0)
            print(f"æƒé‡ä¼˜åŒ–å®Œæˆï¼Œä½¿ç”¨äº† {len(all_weights)} å¸§çš„å¹³å‡æƒé‡")
            print(f"æœ€ç»ˆæƒé‡çŸ©é˜µå½¢çŠ¶: {self.skinning_weights.shape}")
            
            # è®¡ç®—æƒé‡ç»Ÿè®¡ä¿¡æ¯
            weights_per_vertex = np.sum(self.skinning_weights > 0.01, axis=1)  # æ¯ä¸ªé¡¶ç‚¹å—å½±å“çš„å…³èŠ‚æ•°
            print(f"å¹³å‡æ¯ä¸ªé¡¶ç‚¹å— {np.mean(weights_per_vertex):.2f} ä¸ªå…³èŠ‚å½±å“")
            print(f"æƒé‡ç¨€ç–åº¦: {np.mean(self.skinning_weights > 0.01):.3f}")
        else:
            print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸä¼˜åŒ–ä»»ä½•å¸§çš„æƒé‡")
            return None
        
        return self.skinning_weights
    
    def validate_skinning_weights(self, test_frames=None):
        """
        éªŒè¯skinningæƒé‡çš„æ•ˆæœ
        
        Args:
            test_frames: æµ‹è¯•å¸§åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæµ‹è¯•æ‰€æœ‰å¸§
            
        Returns:
            validation_results: éªŒè¯ç»“æœå­—å…¸
        """
        if self.skinning_weights is None:
            print("é”™è¯¯: è¿˜æ²¡æœ‰è®¡ç®—skinningæƒé‡ï¼Œè¯·å…ˆè°ƒç”¨optimize_reference_frame_skinning")
            return None
        
        if test_frames is None:
            test_frames = list(range(min(len(self.mesh_files), 20)))  # é™åˆ¶æµ‹è¯•å¸§æ•°
        
        results = {
            'frame_errors': {},
            'average_error': 0.0,
            'max_error': 0.0,
            'min_error': float('inf')
        }
        
        print("éªŒè¯skinningæƒé‡æ•ˆæœ...")
        
        rest_vertices_norm = self.normalize_mesh_vertices(
            self.rest_pose_vertices, 
            self.frame_normalization_params[self.reference_frame_idx]
        )
        
        total_error = 0.0
        valid_frames = 0
        
        for frame_idx in tqdm(test_frames, desc="éªŒè¯å¸§"):
            if frame_idx >= len(self.mesh_files):
                continue
                
            # åŠ è½½ç›®æ ‡ç½‘æ ¼
            target_mesh = o3d.io.read_triangle_mesh(str(self.mesh_files[frame_idx]))
            target_vertices = np.asarray(target_mesh.vertices)
            
            # å½’ä¸€åŒ–
            if frame_idx not in self.frame_normalization_params:
                self.frame_normalization_params[frame_idx] = self.compute_mesh_normalization_params(target_mesh)
            
            target_vertices_norm = self.normalize_mesh_vertices(
                target_vertices, 
                self.frame_normalization_params[frame_idx]
            )
            
            # è®¡ç®—ç›¸å¯¹å˜æ¢
            target_transforms = self.transforms[frame_idx]
            rest_transforms = self.transforms[self.reference_frame_idx]
            
            relative_transforms = np.zeros_like(target_transforms)
            for j in range(self.num_joints):
                if np.linalg.det(rest_transforms[j][:3, :3]) > 1e-6:
                    rest_inv = np.linalg.inv(rest_transforms[j])
                    relative_transforms[j] = target_transforms[j] @ rest_inv
                else:
                    relative_transforms[j] = np.eye(4)
            
            # ä½¿ç”¨LBSé¢„æµ‹é¡¶ç‚¹ä½ç½®
            predicted_vertices = self.apply_lbs_transform(
                rest_vertices_norm, self.skinning_weights, relative_transforms
            )
            
            # è®¡ç®—è¯¯å·®
            vertex_errors = np.linalg.norm(predicted_vertices - target_vertices_norm, axis=1)
            frame_error = np.mean(vertex_errors)
            
            results['frame_errors'][frame_idx] = {
                'mean_error': frame_error,
                'max_error': np.max(vertex_errors),
                'min_error': np.min(vertex_errors),
                'std_error': np.std(vertex_errors)
            }
            
            total_error += frame_error
            valid_frames += 1
            
            results['max_error'] = max(results['max_error'], frame_error)
            results['min_error'] = min(results['min_error'], frame_error)
        
        if valid_frames > 0:
            results['average_error'] = total_error / valid_frames
            
            print(f"éªŒè¯å®Œæˆï¼")
            print(f"å¹³å‡é‡å»ºè¯¯å·®: {results['average_error']:.6f}")
            print(f"æœ€å¤§è¯¯å·®: {results['max_error']:.6f}")
            print(f"æœ€å°è¯¯å·®: {results['min_error']:.6f}")
        
        return results
    
    def test_lbs_reconstruction_quality(self, test_frames=None, save_meshes=False, output_dir="output/lbs_test"):
        """
        æµ‹è¯•LBSé‡å»ºè´¨é‡çš„è¯¦ç»†æ–¹æ³•
        
        Args:
            test_frames: æµ‹è¯•å¸§åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
            save_meshes: æ˜¯å¦ä¿å­˜é‡å»ºçš„ç½‘æ ¼
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            detailed_results: è¯¦ç»†æµ‹è¯•ç»“æœ
        """
        import time
        import matplotlib.pyplot as plt
        
        if self.skinning_weights is None:
            print("é”™è¯¯: éœ€è¦å…ˆåŠ è½½æˆ–è®¡ç®—skinningæƒé‡")
            return None
        
        # è‡ªåŠ¨é€‰æ‹©æµ‹è¯•å¸§
        if test_frames is None:
            total_frames = len(self.mesh_files)
            if total_frames <= 20:
                test_frames = list(range(total_frames))
            else:
                # é€‰æ‹©ä»£è¡¨æ€§å¸§ï¼šå¼€å§‹ã€ä¸­é—´ã€ç»“æŸï¼Œä»¥åŠä¸€äº›éšæœºå¸§
                test_frames = []
                test_frames.extend([0, 1, 2])  # å¼€å§‹å‡ å¸§
                test_frames.extend([total_frames//4, total_frames//2, 3*total_frames//4])  # ä¸­é—´å¸§
                test_frames.extend([total_frames-3, total_frames-2, total_frames-1])  # ç»“æŸå‡ å¸§
                # æ·»åŠ ä¸€äº›éšæœºå¸§
                import random
                random_frames = random.sample(range(3, total_frames-3), min(6, total_frames-9))
                test_frames.extend(random_frames)
                test_frames = sorted(list(set(test_frames)))  # å»é‡æ’åº
        
        print(f"ğŸ” æµ‹è¯•LBSé‡å»ºè´¨é‡")
        print(f"æµ‹è¯•å¸§: {test_frames}")
        print(f"å‚è€ƒå¸§: {self.reference_frame_idx}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if save_meshes:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡rest poseæ•°æ®
        rest_vertices_norm = self.normalize_mesh_vertices(
            self.rest_pose_vertices, 
            self.frame_normalization_params[self.reference_frame_idx]
        )
        
        # æµ‹è¯•ç»“æœ
        detailed_results = {
            'test_config': {
                'test_frames': test_frames,
                'reference_frame': self.reference_frame_idx,
                'num_vertices': len(rest_vertices_norm),
                'num_joints': self.num_joints,
                'save_meshes': save_meshes,
                'output_dir': str(output_dir) if save_meshes else None
            },
            'frame_results': {},
            'summary_stats': {},
            'performance_stats': {}
        }
        
        all_errors = []
        all_times = []
        distance_errors = []  # è¯¯å·®ä¸è·ç¦»å‚è€ƒå¸§çš„å…³ç³»
        
        print(f"\nå¼€å§‹æµ‹è¯• {len(test_frames)} å¸§...")
        
        for i, frame_idx in enumerate(tqdm(test_frames, desc="æµ‹è¯•é‡å»ºè´¨é‡")):
            if frame_idx >= len(self.mesh_files):
                continue
            
            # åŠ è½½ç›®æ ‡ç½‘æ ¼
            target_mesh = o3d.io.read_triangle_mesh(str(self.mesh_files[frame_idx]))
            target_vertices = np.asarray(target_mesh.vertices)
            
            # å½’ä¸€åŒ–
            if frame_idx not in self.frame_normalization_params:
                self.frame_normalization_params[frame_idx] = self.compute_mesh_normalization_params(target_mesh)
            
            target_vertices_norm = self.normalize_mesh_vertices(
                target_vertices, 
                self.frame_normalization_params[frame_idx]
            )
            
            # è®¡ç®—ç›¸å¯¹å˜æ¢
            target_transforms = self.transforms[frame_idx]
            rest_transforms = self.transforms[self.reference_frame_idx]
            
            relative_transforms = np.zeros_like(target_transforms)
            for j in range(self.num_joints):
                if np.linalg.det(rest_transforms[j][:3, :3]) > 1e-6:
                    rest_inv = np.linalg.inv(rest_transforms[j])
                    relative_transforms[j] = target_transforms[j] @ rest_inv
                else:
                    relative_transforms[j] = np.eye(4)
            
            # LBSé‡å»ºæµ‹è¯•
            start_time = time.time()
            predicted_vertices = self.apply_lbs_transform(
                rest_vertices_norm, self.skinning_weights, relative_transforms
            )
            lbs_time = time.time() - start_time
            
            # è®¡ç®—è¯¦ç»†è¯¯å·®æŒ‡æ ‡
            vertex_errors = np.linalg.norm(predicted_vertices - target_vertices_norm, axis=1)
            
            frame_result = {
                'frame_idx': frame_idx,
                'distance_from_ref': abs(frame_idx - self.reference_frame_idx),
                'mean_error': float(np.mean(vertex_errors)),
                'median_error': float(np.median(vertex_errors)),
                'std_error': float(np.std(vertex_errors)),
                'min_error': float(np.min(vertex_errors)),
                'max_error': float(np.max(vertex_errors)),
                'rmse': float(np.sqrt(np.mean(vertex_errors**2))),
                'p90_error': float(np.percentile(vertex_errors, 90)),
                'p95_error': float(np.percentile(vertex_errors, 95)),
                'p99_error': float(np.percentile(vertex_errors, 99)),
                'lbs_time': lbs_time,
                'vertices_with_large_error': int(np.sum(vertex_errors > 0.05)),  # å¤§è¯¯å·®é¡¶ç‚¹æ•°
                'error_ratio_large': float(np.sum(vertex_errors > 0.05) / len(vertex_errors))  # å¤§è¯¯å·®æ¯”ä¾‹
            }
            
            detailed_results['frame_results'][frame_idx] = frame_result
            
            # æ”¶é›†ç»Ÿè®¡æ•°æ®
            all_errors.extend(vertex_errors)
            all_times.append(lbs_time)
            distance_errors.append((frame_result['distance_from_ref'], frame_result['mean_error']))
            
            # ä¿å­˜ç½‘æ ¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if save_meshes:
                # ä¿å­˜é‡å»ºç½‘æ ¼
                reconstructed_mesh = o3d.geometry.TriangleMesh()
                reconstructed_mesh.vertices = o3d.utility.Vector3dVector(predicted_vertices)
                if hasattr(target_mesh, 'triangles') and len(target_mesh.triangles) > 0:
                    reconstructed_mesh.triangles = target_mesh.triangles
                
                mesh_output_path = output_path / f"frame_{frame_idx:06d}_reconstructed.obj"
                o3d.io.write_triangle_mesh(str(mesh_output_path), reconstructed_mesh)
                
                # ä¿å­˜è¯¯å·®å¯è§†åŒ–ç½‘æ ¼
                normalized_errors = vertex_errors / np.max(vertex_errors)
                error_colors = plt.cm.plasma(normalized_errors)[:, :3]  # ä½¿ç”¨plasmaé¢œè‰²æ˜ å°„
                
                error_mesh = o3d.geometry.TriangleMesh()
                error_mesh.vertices = target_mesh.vertices
                error_mesh.triangles = target_mesh.triangles
                error_mesh.vertex_colors = o3d.utility.Vector3dVector(error_colors)
                
                error_output_path = output_path / f"frame_{frame_idx:06d}_error_colored.obj"
                o3d.io.write_triangle_mesh(str(error_output_path), error_mesh)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        if all_errors:
            all_errors = np.array(all_errors)
            detailed_results['summary_stats'] = {
                'total_tested_frames': len(test_frames),
                'total_vertices': len(all_errors),
                'overall_mean_error': float(np.mean(all_errors)),
                'overall_median_error': float(np.median(all_errors)),
                'overall_std_error': float(np.std(all_errors)),
                'overall_min_error': float(np.min(all_errors)),
                'overall_max_error': float(np.max(all_errors)),
                'overall_rmse': float(np.sqrt(np.mean(all_errors**2))),
                'overall_p90': float(np.percentile(all_errors, 90)),
                'overall_p95': float(np.percentile(all_errors, 95)),
                'overall_p99': float(np.percentile(all_errors, 99)),
                'vertices_with_large_error_total': int(np.sum(all_errors > 0.05)),
                'large_error_ratio': float(np.sum(all_errors > 0.05) / len(all_errors))
            }
        
        # æ€§èƒ½ç»Ÿè®¡
        if all_times:
            detailed_results['performance_stats'] = {
                'mean_lbs_time': float(np.mean(all_times)),
                'total_lbs_time': float(np.sum(all_times)),
                'min_lbs_time': float(np.min(all_times)),
                'max_lbs_time': float(np.max(all_times)),
                'fps_estimate': float(len(all_times) / np.sum(all_times)) if np.sum(all_times) > 0 else 0
            }
        
        # åˆ†æè¯¯å·®ä¸è·ç¦»çš„å…³ç³»
        if distance_errors:
            distances, errors = zip(*distance_errors)
            if len(set(distances)) > 1:  # æœ‰ä¸åŒè·ç¦»çš„æ•°æ®ç‚¹
                correlation = np.corrcoef(distances, errors)[0, 1]
                detailed_results['distance_analysis'] = {
                    'correlation_with_distance': float(correlation),
                    'distance_error_pairs': distance_errors
                }
        
        # è¾“å‡ºæ€»ç»“
        print(f"\nğŸ“Š æµ‹è¯•å®Œæˆæ€»ç»“:")
        if 'summary_stats' in detailed_results:
            stats = detailed_results['summary_stats']
            print(f"æ€»ä½“å¹³å‡è¯¯å·®: {stats['overall_mean_error']:.6f}")
            print(f"æ€»ä½“RMSE: {stats['overall_rmse']:.6f}")
            print(f"è¯¯å·®èŒƒå›´: [{stats['overall_min_error']:.6f}, {stats['overall_max_error']:.6f}]")
            print(f"å¤§è¯¯å·®é¡¶ç‚¹æ¯”ä¾‹: {stats['large_error_ratio']*100:.2f}%")
        
        if 'performance_stats' in detailed_results:
            perf = detailed_results['performance_stats']
            print(f"å¹³å‡LBSæ—¶é—´: {perf['mean_lbs_time']:.3f}s")
            print(f"ä¼°è®¡å¸§ç‡: {perf['fps_estimate']:.1f} FPS")
        
        if 'distance_analysis' in detailed_results:
            dist_analysis = detailed_results['distance_analysis']
            print(f"è¯¯å·®ä¸è·ç¦»å‚è€ƒå¸§ç›¸å…³æ€§: {dist_analysis['correlation_with_distance']:.3f}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if save_meshes:
            import json
            results_path = output_path / "test_results.json"
            
            # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç»“æœ
            serializable_results = detailed_results.copy()
            # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„éƒ¨åˆ†
            if 'distance_analysis' in serializable_results:
                serializable_results['distance_analysis'] = {
                    'correlation_with_distance': detailed_results['distance_analysis']['correlation_with_distance']
                }
            
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜:")
            print(f"   æµ‹è¯•ç»“æœ: {results_path}")
            print(f"   é‡å»ºç½‘æ ¼: {output_path}/*_reconstructed.obj")
            print(f"   è¯¯å·®å¯è§†åŒ–: {output_path}/*_error_colored.obj")
        
        return detailed_results
    
    def save_skinning_weights(self, output_path):
        """
        ä¿å­˜skinningæƒé‡
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if self.skinning_weights is None:
            print("é”™è¯¯: æ²¡æœ‰å¯ä¿å­˜çš„skinningæƒé‡")
            return
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æƒé‡å’Œç›¸å…³ä¿¡æ¯
        skinning_data = {
            'weights': self.skinning_weights,
            'rest_vertices': self.rest_pose_vertices,
            'rest_transforms': self.rest_pose_transforms,
            'reference_frame_idx': self.reference_frame_idx,
            'num_vertices': self.skinning_weights.shape[0],
            'num_joints': self.skinning_weights.shape[1]
        }
        
        np.savez_compressed(output_path, **skinning_data)
        print(f"Skinningæƒé‡å·²ä¿å­˜åˆ°: {output_path}")
    
    def load_skinning_weights(self, input_path):
        """
        åŠ è½½skinningæƒé‡
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        """
        input_path = Path(input_path)
        if not input_path.exists():
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return False
        
        try:
            data = np.load(input_path)
            self.skinning_weights = data['weights']
            self.rest_pose_vertices = data['rest_vertices']
            self.rest_pose_transforms = data['rest_transforms']
            
            print(f"æˆåŠŸåŠ è½½skinningæƒé‡:")
            print(f"  - æƒé‡çŸ©é˜µå½¢çŠ¶: {self.skinning_weights.shape}")
            print(f"  - Rest poseé¡¶ç‚¹æ•°: {len(self.rest_pose_vertices)}")
            print(f"  - Reference frame: {data['reference_frame_idx']}")
            
            return True
        except Exception as e:
            print(f"åŠ è½½skinningæƒé‡å¤±è´¥: {e}")
            return False
    
    def find_vertex_correspondence(self, target_mesh, target_frame_idx):
        """
        æ‰¾åˆ°ç›®æ ‡ç½‘æ ¼ä¸å‚è€ƒç½‘æ ¼çš„é¡¶ç‚¹å¯¹åº”å…³ç³»ï¼ˆåŸå§‹ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼‰
        
        Args:
            target_mesh: ç›®æ ‡ç½‘æ ¼
            target_frame_idx: ç›®æ ‡å¸§ç´¢å¼•
            
        Returns:
            correspondence_map: ä»ç›®æ ‡é¡¶ç‚¹ç´¢å¼•åˆ°å‚è€ƒé¡¶ç‚¹ç´¢å¼•çš„æ˜ å°„
        """
        # è®¡ç®—ç‰¹å¾
        ref_features = self.compute_vertex_features(self.reference_mesh, self.reference_frame_idx)
        target_features = self.compute_vertex_features(target_mesh, target_frame_idx)
        
        print(f"å‚è€ƒç‰¹å¾ç»´åº¦: {ref_features.shape}, ç›®æ ‡ç‰¹å¾ç»´åº¦: {target_features.shape}")
        
        # ä½¿ç”¨æœ€è¿‘é‚»åŒ¹é…
        nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(ref_features)
        distances, indices = nbrs.kneighbors(target_features)
        
        correspondence_map = indices.flatten()
        
        # è®¡ç®—åŒ¹é…è´¨é‡
        avg_distance = np.mean(distances)
        print(f"å¸§ {target_frame_idx} å¹³å‡åŒ¹é…è·ç¦»: {avg_distance:.4f}")
        
        return correspondence_map
    
    def reorder_mesh_vertices(self, mesh, correspondence_map):
        """
        æ ¹æ®å¯¹åº”å…³ç³»é‡æ–°æ’åºç½‘æ ¼é¡¶ç‚¹
        
        Args:
            mesh: è¾“å…¥ç½‘æ ¼
            correspondence_map: é¡¶ç‚¹å¯¹åº”å…³ç³»æ˜ å°„
            
        Returns:
            reordered_mesh: é‡æ–°æ’åºçš„ç½‘æ ¼
        """
        # åˆ›å»ºæ–°çš„é¡¶ç‚¹æ’åˆ—
        new_vertices = np.zeros_like(np.asarray(self.reference_mesh.vertices))
        
        for target_idx, ref_idx in enumerate(correspondence_map):
            if target_idx < len(mesh.vertices):
                new_vertices[ref_idx] = np.asarray(mesh.vertices)[target_idx]
        
        # åˆ›å»ºæ–°çš„ç½‘æ ¼
        reordered_mesh = o3d.geometry.TriangleMesh()
        reordered_mesh.vertices = o3d.utility.Vector3dVector(new_vertices)
        
        # å¤åˆ¶é¢ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(mesh, 'triangles') and len(mesh.triangles) > 0:
            reordered_mesh.triangles = o3d.utility.Vector3iVector(np.asarray(mesh.triangles))
        
        # å¤åˆ¶å…¶ä»–å±æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(mesh, 'vertex_normals') and len(mesh.vertex_normals) > 0:
            reordered_mesh.vertex_normals = mesh.vertex_normals
        if hasattr(mesh, 'vertex_colors') and len(mesh.vertex_colors) > 0:
            reordered_mesh.vertex_colors = mesh.vertex_colors
            
        return reordered_mesh
    
    def optimize_correspondence_with_temporal_consistency(self, mesh_sequence_subset=None, max_frames=10, use_skeleton_driven=True):
        """
        ä½¿ç”¨æ—¶é—´ä¸€è‡´æ€§ä¼˜åŒ–å¯¹åº”å…³ç³»
        
        Args:
            mesh_sequence_subset: ç½‘æ ¼åºåˆ—å­é›†, å¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰
            max_frames: æœ€å¤§å¤„ç†å¸§æ•°
            use_skeleton_driven: æ˜¯å¦ä½¿ç”¨éª¨éª¼é©±åŠ¨çš„å¿«é€Ÿæ–¹æ³•
        """
        if mesh_sequence_subset is None:
            frame_indices = list(range(min(len(self.mesh_files), max_frames)))
        else:
            frame_indices = mesh_sequence_subset
        
        # åˆå§‹åŒ–å¯¹åº”å…³ç³»
        correspondences = {}
        meshes = {}
        
        # åŠ è½½ç½‘æ ¼å¹¶è®¡ç®—åˆå§‹å¯¹åº”å…³ç³»
        method_name = "éª¨éª¼é©±åŠ¨" if use_skeleton_driven else "ç‰¹å¾åŒ¹é…"
        print(f"å¼€å§‹è®¡ç®—åŸºäº{method_name}çš„é¡¶ç‚¹å¯¹åº”å…³ç³»...")
        
        for i, frame_idx in enumerate(tqdm(frame_indices, desc="è®¡ç®—åˆå§‹å¯¹åº”å…³ç³»")):
            if frame_idx == self.reference_frame_idx:
                # å‚è€ƒå¸§ä½¿ç”¨æ’ç­‰æ˜ å°„
                correspondences[frame_idx] = np.arange(len(self.reference_mesh.vertices))
                meshes[frame_idx] = self.reference_mesh
            else:
                mesh = o3d.io.read_triangle_mesh(str(self.mesh_files[frame_idx]))
                meshes[frame_idx] = mesh
                
                if use_skeleton_driven:
                    # ä½¿ç”¨éª¨éª¼é©±åŠ¨çš„å¿«é€Ÿå¯¹åº”å…³ç³»è®¡ç®—
                    correspondences[frame_idx] = self.compute_skeleton_driven_correspondence(mesh, frame_idx)
                else:
                    # ä½¿ç”¨åŸå§‹çš„ç‰¹å¾åŒ¹é…æ–¹æ³•
                    correspondences[frame_idx] = self.find_vertex_correspondence(mesh, frame_idx)
        
        # æ—¶é—´ä¸€è‡´æ€§ä¼˜åŒ–
        print("è¿›è¡Œæ—¶é—´ä¸€è‡´æ€§ä¼˜åŒ–...")
        for iteration in range(3):  # è¿­ä»£ä¼˜åŒ–
            for i, frame_idx in enumerate(frame_indices):
                if frame_idx == self.reference_frame_idx:
                    continue
                
                # è·å–ç›¸é‚»å¸§
                prev_frame = frame_indices[max(0, i-1)]
                next_frame = frame_indices[min(len(frame_indices)-1, i+1)]
                
                if prev_frame != frame_idx and next_frame != frame_idx:
                    # ä½¿ç”¨ç›¸é‚»å¸§ä¿¡æ¯è°ƒæ•´å½“å‰å¸§çš„å¯¹åº”å…³ç³»
                    prev_corr = correspondences[prev_frame]
                    next_corr = correspondences[next_frame]
                    current_corr = correspondences[frame_idx]
                    
                    # ç®€å•çš„æ—¶é—´ä¸€è‡´æ€§çº¦æŸï¼šå½“å‰å¯¹åº”å…³ç³»åº”è¯¥æ¥è¿‘ç›¸é‚»å¸§çš„å¹³å‡
                    # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ä¼˜åŒ–ç®—æ³•
                    pass
        
        return correspondences, meshes
    
    def canonicalize_mesh_sequence(self, output_folder, max_frames=None, use_skeleton_driven=True):
        """
        å¯¹æ•´ä¸ªç½‘æ ¼åºåˆ—è¿›è¡Œç»Ÿä¸€åŒ–
        
        Args:
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
            max_frames: æœ€å¤§å¤„ç†å¸§æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
            use_skeleton_driven: æ˜¯å¦ä½¿ç”¨éª¨éª¼é©±åŠ¨çš„å¿«é€Ÿå¯¹åº”å…³ç³»è®¡ç®—
        """
        output_path = Path(output_folder)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç¡®å®šå¤„ç†çš„å¸§æ•°
        if max_frames is None:
            max_frames = len(self.mesh_files)
        else:
            max_frames = min(max_frames, len(self.mesh_files))
        
        frame_indices = list(range(max_frames))
        
        # ä¼˜åŒ–å¯¹åº”å…³ç³»
        correspondences, meshes = self.optimize_correspondence_with_temporal_consistency(
            frame_indices, max_frames, use_skeleton_driven
        )
        
        # ä¿å­˜ç»Ÿä¸€åŒ–çš„ç½‘æ ¼
        print("ä¿å­˜ç»Ÿä¸€åŒ–ç½‘æ ¼...")
        canonicalized_info = {
            'reference_frame': self.reference_frame_idx,
            'correspondences': {},
            'stats': {}
        }
        
        for frame_idx in tqdm(frame_indices, desc="ä¿å­˜ç½‘æ ¼"):
            mesh = meshes[frame_idx]
            correspondence = correspondences[frame_idx]
            
            # é‡æ–°æ’åºé¡¶ç‚¹
            if frame_idx != self.reference_frame_idx:
                canonical_mesh = self.reorder_mesh_vertices(mesh, correspondence)
            else:
                canonical_mesh = mesh
            
            # ä¿å­˜ç½‘æ ¼
            output_file = output_path / f"canonical_frame_{frame_idx:06d}.obj"
            success = o3d.io.write_triangle_mesh(str(output_file), canonical_mesh)
            if not success:
                print(f"è­¦å‘Š: ä¿å­˜ç½‘æ ¼æ–‡ä»¶å¤±è´¥: {output_file}")
            
            # ä¿å­˜å¯¹åº”å…³ç³»ä¿¡æ¯
            canonicalized_info['correspondences'][str(frame_idx)] = correspondence.tolist()
            canonicalized_info['stats'][str(frame_idx)] = {
                'original_vertices': len(mesh.vertices),
                'canonical_vertices': len(canonical_mesh.vertices)
            }
        
        # ä¿å­˜å…ƒä¿¡æ¯
        with open(output_path / 'canonicalization_info.json', 'w') as f:
            json.dump(canonicalized_info, f, indent=2)
        
        print(f"ç»Ÿä¸€åŒ–å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_path}")
        return canonicalized_info

def main():
    """
    ä¸»å‡½æ•°ç¤ºä¾‹
    """
    # é…ç½®è·¯å¾„
    skeleton_data_dir = "output/skeleton_prediction"    # åŒ…å«npyæ–‡ä»¶çš„æ–‡ä»¶å¤¹
    mesh_folder_path = "D:/Code/VVEditor/Rafa_Approves_hd_4k"            # åŒ…å«objæ–‡ä»¶çš„æ–‡ä»¶å¤¹
    output_folder = "output/canonical_meshes"         # è¾“å‡ºæ–‡ä»¶å¤¹

    # åˆ›å»ºç»Ÿä¸€åŒ–å™¨
    canonicalizer = InverseMeshCanonicalizer(
        skeleton_data_dir=skeleton_data_dir,
        reference_frame_idx=5  # ä½¿ç”¨ç¬¬5å¸§ä½œä¸ºå‚è€ƒ
    )
    
    # åŠ è½½ç½‘æ ¼åºåˆ—
    canonicalizer.load_mesh_sequence(mesh_folder_path)
    
    print("=" * 60)
    print("å¼€å§‹LBSæƒé‡ä¼˜åŒ–...")
    print("=" * 60)
    
    # ä¼˜åŒ–LBSæƒé‡
    skinning_weights = canonicalizer.optimize_reference_frame_skinning(
        regularization_lambda=0.01,
        max_iter=500
    )
    
    if skinning_weights is not None:
        # ä¿å­˜æƒé‡
        weights_output_path = "output/skinning_weights.npz"
        canonicalizer.save_skinning_weights(weights_output_path)
        
        print("=" * 60)
        print("éªŒè¯LBSæƒé‡æ•ˆæœ...")
        print("=" * 60)
        
        # éªŒè¯æƒé‡æ•ˆæœ
        validation_results = canonicalizer.validate_skinning_weights(
            test_frames=list(range(0, min(len(canonicalizer.mesh_files), 15), 2))  # æµ‹è¯•éƒ¨åˆ†å¸§
        )
        
        if validation_results:
            print(f"\néªŒè¯ç»“æœæ‘˜è¦:")
            print(f"å¹³å‡é‡å»ºè¯¯å·®: {validation_results['average_error']:.6f}")
            print(f"è¯¯å·®èŒƒå›´: [{validation_results['min_error']:.6f}, {validation_results['max_error']:.6f}]")
            
            # æ˜¾ç¤ºæ¯å¸§çš„è¯¦ç»†è¯¯å·®
            print(f"\nå„å¸§è¯¦ç»†è¯¯å·®:")
            for frame_idx, frame_result in validation_results['frame_errors'].items():
                print(f"  å¸§ {frame_idx:2d}: {frame_result['mean_error']:.6f} "
                      f"(std: {frame_result['std_error']:.6f})")
    
    print("=" * 60)
    print("å¯é€‰: æ‰§è¡Œä¼ ç»Ÿç½‘æ ¼ç»Ÿä¸€åŒ–...")
    print("=" * 60)
    
    # å¯é€‰ï¼šä¹Ÿå¯ä»¥æ‰§è¡Œä¼ ç»Ÿçš„ç½‘æ ¼ç»Ÿä¸€åŒ–è¿›è¡Œæ¯”è¾ƒ
    canonicalization_info = canonicalizer.canonicalize_mesh_sequence(
        output_folder=output_folder,
        max_frames=10,  # é™åˆ¶å¤„ç†å¸§æ•°ä»¥åŠ å¿«æµ‹è¯•
        use_skeleton_driven=True  # ä½¿ç”¨éª¨éª¼é©±åŠ¨æ–¹æ³•
    )
    
    print("=" * 60)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"LBSæƒé‡ä¼˜åŒ–ç»“æœä¿å­˜åœ¨: output/skinning_weights.npz")
    print(f"ä¼ ç»Ÿç»Ÿä¸€åŒ–ç»“æœä¿å­˜åœ¨: {output_folder}")
    print(f"LBSæ–¹æ³•å¤„ç†äº†reference frameçš„skinningæƒé‡ä¼˜åŒ–")
    print(f"ä¼ ç»Ÿæ–¹æ³•å¤„ç†äº† {len(canonicalization_info['correspondences'])} å¸§çš„ç½‘æ ¼ç»Ÿä¸€åŒ–")


def demo_lbs_only():
    """
    ä»…æ¼”ç¤ºLBSæƒé‡ä¼˜åŒ–çš„å‡½æ•°
    """
    print("=" * 60)
    print("LBSæƒé‡ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # é…ç½®è·¯å¾„
    skeleton_data_dir = "output/skeleton_prediction"
    mesh_folder_path = "D:/Code/VVEditor/Rafa_Approves_hd_4k"
    
    # åˆ›å»ºç»Ÿä¸€åŒ–å™¨
    canonicalizer = InverseMeshCanonicalizer(
        skeleton_data_dir=skeleton_data_dir,
        reference_frame_idx=5
    )
    
    # åŠ è½½ç½‘æ ¼åºåˆ—
    canonicalizer.load_mesh_sequence(mesh_folder_path)
    
    # ä¼˜åŒ–æƒé‡
    skinning_weights = canonicalizer.optimize_reference_frame_skinning(
        regularization_lambda=0.01,
        max_iter=300
    )
    
    if skinning_weights is not None:
        # ä¿å­˜å’ŒéªŒè¯
        canonicalizer.save_skinning_weights("output/skinning_weights.npz")
        validation_results = canonicalizer.validate_skinning_weights()
        
        print(f"\nLBSä¼˜åŒ–å®Œæˆï¼")
        print(f"å¹³å‡é‡å»ºè¯¯å·®: {validation_results['average_error']:.6f}")
    
    return canonicalizer

if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    mode = "lbs_only"  # å¯é€‰: "full", "lbs_only"
    
    if mode == "lbs_only":
        # ä»…è¿è¡ŒLBSæƒé‡ä¼˜åŒ–
        canonicalizer = demo_lbs_only()
    else:
        # è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆLBS + ä¼ ç»Ÿç»Ÿä¸€åŒ–ï¼‰
        main()