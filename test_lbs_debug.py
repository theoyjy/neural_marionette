#!/usr/bin/env python3
"""
æœ€ç®€å•çš„LBSæƒé‡ä¼˜åŒ–æµ‹è¯•

è¿™ä¸ªç‰ˆæœ¬åªæµ‹è¯•æœ€åŸºæœ¬çš„åŠŸèƒ½ï¼Œæ²¡æœ‰å¤æ‚çš„ä¼˜åŒ–
"""

import numpy as np
import sys
from pathlib import Path
from Skinning import InverseMeshCanonicalizer

def test_basic_lbs():
    """
    æœ€åŸºæœ¬çš„LBSæµ‹è¯• - åªæµ‹è¯•LBSå˜æ¢æœ¬èº«
    """
    print("=" * 80)
    print("åŸºæœ¬LBSå˜æ¢æµ‹è¯•")
    print("=" * 80)
    
    skeleton_data_dir = "output/skeleton_prediction"
    mesh_folder_path = "D:/Code/VVEditor/Rafa_Approves_hd_4k"
    
    try:
        # åˆ›å»ºç»Ÿä¸€åŒ–å™¨
        canonicalizer = InverseMeshCanonicalizer(
            skeleton_data_dir=skeleton_data_dir,
            reference_frame_idx=0
        )
        
        # åŠ è½½ç½‘æ ¼åºåˆ—
        canonicalizer.load_mesh_sequence(mesh_folder_path)
        
        print(f"é¡¶ç‚¹æ•°: {len(canonicalizer.reference_mesh.vertices)}")
        print(f"å…³èŠ‚æ•°: {canonicalizer.num_joints}")
        
        # è®¾ç½®rest pose
        rest_vertices = np.asarray(canonicalizer.reference_mesh.vertices)
        
        # è®¡ç®—å½’ä¸€åŒ–å‚æ•°å¹¶å½’ä¸€åŒ–é¡¶ç‚¹
        rest_norm_params = canonicalizer.compute_mesh_normalization_params(canonicalizer.reference_mesh)
        rest_vertices_norm = canonicalizer.normalize_mesh_vertices(rest_vertices, rest_norm_params)
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æƒé‡ï¼ˆåŸºäºè·ç¦»ï¼‰
        keypoints = canonicalizer.keypoints[0, :, :3]  # reference frameçš„å…³èŠ‚ç‚¹
        
        print("åˆ›å»ºåŸºäºè·ç¦»çš„æƒé‡...")
        distances = np.linalg.norm(rest_vertices_norm[:, None, :] - keypoints[None, :, :], axis=2)  # [V, J]
        weights = np.exp(-distances**2 / (2 * 0.1**2))
        weights = weights / (np.sum(weights, axis=1, keepdims=True) + 1e-8)  # å½’ä¸€åŒ–
        
        print(f"æƒé‡çŸ©é˜µå½¢çŠ¶: {weights.shape}")
        print(f"æƒé‡ç»Ÿè®¡: min={np.min(weights):.4f}, max={np.max(weights):.4f}, mean={np.mean(weights):.4f}")
        
        # æµ‹è¯•LBSå˜æ¢
        print("\næµ‹è¯•LBSå˜æ¢...")
        rest_transforms = canonicalizer.transforms[0]  # reference frame
        target_transforms = canonicalizer.transforms[1]  # target frame
        
        # è®¡ç®—ç›¸å¯¹å˜æ¢
        relative_transforms = np.zeros_like(target_transforms)
        for j in range(canonicalizer.num_joints):
            try:
                rest_inv = np.linalg.inv(rest_transforms[j])
                relative_transforms[j] = target_transforms[j] @ rest_inv
            except:
                relative_transforms[j] = np.eye(4)
        
        # åº”ç”¨LBSå˜æ¢
        transformed_vertices = canonicalizer.apply_lbs_transform(rest_vertices_norm, weights, relative_transforms)
        
        print(f"å˜æ¢ç»“æœå½¢çŠ¶: {transformed_vertices.shape}")
        print(f"å˜æ¢å·®å¼‚ç»Ÿè®¡:")
        diff = transformed_vertices - rest_vertices_norm
        print(f"  - å¹³å‡å·®å¼‚: {np.mean(np.linalg.norm(diff, axis=1)):.6f}")
        print(f"  - æœ€å¤§å·®å¼‚: {np.max(np.linalg.norm(diff, axis=1)):.6f}")
        
        # ä¸çœŸå®ç›®æ ‡æ¯”è¾ƒ
        print("\nä¸çœŸå®ç›®æ ‡æ¯”è¾ƒ...")
        target_mesh = canonicalizer.mesh_files[1]
        target_mesh_obj = __import__('open3d').io.read_triangle_mesh(str(target_mesh))
        target_vertices = np.asarray(target_mesh_obj.vertices)
        target_norm_params = canonicalizer.compute_mesh_normalization_params(target_mesh_obj)
        target_vertices_norm = canonicalizer.normalize_mesh_vertices(target_vertices, target_norm_params)
        
        error = np.linalg.norm(transformed_vertices - target_vertices_norm, axis=1)
        print(f"ä¸çœŸå®ç›®æ ‡çš„è¯¯å·®:")
        print(f"  - å¹³å‡è¯¯å·®: {np.mean(error):.6f}")
        print(f"  - æœ€å¤§è¯¯å·®: {np.max(error):.6f}")
        print(f"  - æ ‡å‡†å·®: {np.std(error):.6f}")
        
        print("\nâœ… åŸºæœ¬LBSå˜æ¢æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_optimization():
    """
    ç®€å•ä¼˜åŒ–æµ‹è¯• - åªä¼˜åŒ–å¾ˆå°‘çš„é¡¶ç‚¹
    """
    print("=" * 80)
    print("ç®€å•ä¼˜åŒ–æµ‹è¯•")
    print("=" * 80)
    
    skeleton_data_dir = "output/skeleton_prediction"
    mesh_folder_path = "D:/Code/VVEditor/Rafa_Approves_hd_4k"
    
    try:
        canonicalizer = InverseMeshCanonicalizer(
            skeleton_data_dir=skeleton_data_dir,
            reference_frame_idx=0
        )
        
        canonicalizer.load_mesh_sequence(mesh_folder_path)
        
        # è®¾ç½®rest pose
        canonicalizer.rest_pose_vertices = np.asarray(canonicalizer.reference_mesh.vertices)
        canonicalizer.rest_pose_transforms = canonicalizer.transforms[0]
        
        # åªä½¿ç”¨å‰100ä¸ªé¡¶ç‚¹è¿›è¡Œæµ‹è¯•
        num_test_vertices = 100
        rest_vertices = canonicalizer.rest_pose_vertices[:num_test_vertices]
        
        # è®¡ç®—å½’ä¸€åŒ–
        rest_norm_params = canonicalizer.frame_normalization_params[0]
        rest_vertices_norm = canonicalizer.normalize_mesh_vertices(rest_vertices, rest_norm_params)
        
        # ç›®æ ‡æ•°æ®
        target_frame = 1
        target_mesh = __import__('open3d').io.read_triangle_mesh(str(canonicalizer.mesh_files[target_frame]))
        target_vertices = np.asarray(target_mesh.vertices)[:num_test_vertices]
        target_norm_params = canonicalizer.compute_mesh_normalization_params(target_mesh)
        target_vertices_norm = canonicalizer.normalize_mesh_vertices(target_vertices, target_norm_params)
        
        # ç›¸å¯¹å˜æ¢
        rest_transforms = canonicalizer.transforms[0]
        target_transforms = canonicalizer.transforms[target_frame]
        relative_transforms = np.zeros_like(target_transforms)
        for j in range(canonicalizer.num_joints):
            try:
                rest_inv = np.linalg.inv(rest_transforms[j])
                relative_transforms[j] = target_transforms[j] @ rest_inv
            except:
                relative_transforms[j] = np.eye(4)
        
        # åˆå§‹æƒé‡ï¼ˆåŸºäºè·ç¦»ï¼‰
        keypoints = canonicalizer.keypoints[0, :, :3]
        distances = np.linalg.norm(rest_vertices_norm[:, None, :] - keypoints[None, :, :], axis=2)
        weights_init = np.exp(-distances**2 / (2 * 0.1**2))
        weights_init = weights_init / (np.sum(weights_init, axis=1, keepdims=True) + 1e-8)
        
        print(f"æµ‹è¯•é¡¶ç‚¹æ•°: {num_test_vertices}")
        print(f"å…³èŠ‚æ•°: {canonicalizer.num_joints}")
        print(f"åˆå§‹æƒé‡å½¢çŠ¶: {weights_init.shape}")
        
        # ç®€å•çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
        print("\nå¼€å§‹ç®€å•æ¢¯åº¦ä¸‹é™ä¼˜åŒ–...")
        weights = weights_init.copy()
        learning_rate = 0.001
        num_iterations = 50
        
        for iteration in range(num_iterations):
            # å‰å‘ä¼ æ’­
            predicted = canonicalizer.apply_lbs_transform(rest_vertices_norm, weights, relative_transforms)
            
            # è®¡ç®—æŸå¤±
            error = predicted - target_vertices_norm
            loss = np.mean(np.sum(error**2, axis=1))
            
            if iteration % 10 == 0:
                print(f"  è¿­ä»£ {iteration}: æŸå¤± = {loss:.6f}")
            
            # ç®€å•çš„æ¢¯åº¦è¿‘ä¼¼ï¼ˆæœ‰é™å·®åˆ†ï¼‰
            gradient = np.zeros_like(weights)
            eps = 1e-6
            
            for i in range(min(10, num_test_vertices)):  # åªè®¡ç®—å‰10ä¸ªé¡¶ç‚¹çš„æ¢¯åº¦
                for j in range(canonicalizer.num_joints):
                    # å‰å‘å·®åˆ†
                    weights_plus = weights.copy()
                    weights_plus[i, j] += eps
                    # é‡æ–°å½’ä¸€åŒ–
                    weights_plus[i] = weights_plus[i] / (np.sum(weights_plus[i]) + 1e-8)
                    
                    predicted_plus = canonicalizer.apply_lbs_transform(rest_vertices_norm, weights_plus, relative_transforms)
                    loss_plus = np.mean(np.sum((predicted_plus - target_vertices_norm)**2, axis=1))
                    
                    gradient[i, j] = (loss_plus - loss) / eps
            
            # æ›´æ–°æƒé‡
            weights -= learning_rate * gradient
            
            # ç¡®ä¿éè´Ÿå¹¶é‡æ–°å½’ä¸€åŒ–
            weights = np.maximum(weights, 0)
            weights = weights / (np.sum(weights, axis=1, keepdims=True) + 1e-8)
        
        # æœ€ç»ˆè¯„ä¼°
        final_predicted = canonicalizer.apply_lbs_transform(rest_vertices_norm, weights, relative_transforms)
        final_error = np.mean(np.linalg.norm(final_predicted - target_vertices_norm, axis=1))
        
        print(f"\nâœ… ç®€å•ä¼˜åŒ–å®Œæˆï¼")
        print(f"æœ€ç»ˆè¯¯å·®: {final_error:.6f}")
        print(f"æƒé‡ç¨€ç–åº¦: {np.mean(weights > 0.01):.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def debug_optimization_issue():
    """
    è°ƒè¯•ä¼˜åŒ–é—®é¢˜
    """
    print("=" * 80)
    print("è°ƒè¯•ä¼˜åŒ–é—®é¢˜")
    print("=" * 80)
    
    skeleton_data_dir = "output/skeleton_prediction"
    mesh_folder_path = "D:/Code/VVEditor/Rafa_Approves_hd_4k"
    
    try:
        canonicalizer = InverseMeshCanonicalizer(
            skeleton_data_dir=skeleton_data_dir,
            reference_frame_idx=0
        )
        
        canonicalizer.load_mesh_sequence(mesh_folder_path)
        
        # è®¾ç½®rest pose
        canonicalizer.rest_pose_vertices = np.asarray(canonicalizer.reference_mesh.vertices)
        canonicalizer.rest_pose_transforms = canonicalizer.transforms[0]
        
        print(f"Rest verticeså½¢çŠ¶: {canonicalizer.rest_pose_vertices.shape}")
        print(f"å˜æ¢çŸ©é˜µå½¢çŠ¶: {canonicalizer.transforms.shape}")
        
        # æµ‹è¯•å•ä¸ªä¼˜åŒ–å‡½æ•°çš„ç»„ä»¶
        target_frame = 1
        
        # æ£€æŸ¥æ¯ä¸ªæ­¥éª¤
        print("\n1. æ£€æŸ¥ç½‘æ ¼åŠ è½½...")
        target_mesh = __import__('open3d').io.read_triangle_mesh(str(canonicalizer.mesh_files[target_frame]))
        print(f"ç›®æ ‡ç½‘æ ¼é¡¶ç‚¹æ•°: {len(target_mesh.vertices)}")
        
        print("\n2. æ£€æŸ¥å½’ä¸€åŒ–...")
        target_norm_params = canonicalizer.compute_mesh_normalization_params(target_mesh)
        print(f"å½’ä¸€åŒ–å‚æ•°: {target_norm_params}")
        
        print("\n3. æ£€æŸ¥ç›¸å¯¹å˜æ¢è®¡ç®—...")
        rest_transforms = canonicalizer.transforms[0]
        target_transforms = canonicalizer.transforms[target_frame]
        print(f"Restå˜æ¢çŸ©é˜µç¬¬ä¸€ä¸ªå…³èŠ‚:")
        print(rest_transforms[0])
        print(f"ç›®æ ‡å˜æ¢çŸ©é˜µç¬¬ä¸€ä¸ªå…³èŠ‚:")
        print(target_transforms[0])
        
        # æ£€æŸ¥çŸ©é˜µå¯é€†æ€§
        print("\n4. æ£€æŸ¥çŸ©é˜µå¯é€†æ€§...")
        for j in range(min(5, canonicalizer.num_joints)):
            det = np.linalg.det(rest_transforms[j][:3, :3])
            print(f"å…³èŠ‚ {j} è¡Œåˆ—å¼: {det}")
            if abs(det) < 1e-6:
                print(f"  è­¦å‘Š: å…³èŠ‚ {j} çš„å˜æ¢çŸ©é˜µæ¥è¿‘å¥‡å¼‚!")
        
        print("\n5. æµ‹è¯•LBSæŸå¤±å‡½æ•°...")
        # åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•æ¡ˆä¾‹
        test_vertices = 5
        test_rest = canonicalizer.rest_pose_vertices[:test_vertices]
        test_target = np.asarray(target_mesh.vertices)[:test_vertices]
        
        # å½’ä¸€åŒ–
        rest_norm = canonicalizer.normalize_mesh_vertices(test_rest, canonicalizer.frame_normalization_params[0])
        target_norm = canonicalizer.normalize_mesh_vertices(test_target, target_norm_params)
        
        # è®¡ç®—ç›¸å¯¹å˜æ¢
        relative_transforms = np.zeros_like(target_transforms)
        for j in range(canonicalizer.num_joints):
            if np.linalg.det(rest_transforms[j][:3, :3]) > 1e-6:
                rest_inv = np.linalg.inv(rest_transforms[j])
                relative_transforms[j] = target_transforms[j] @ rest_inv
            else:
                relative_transforms[j] = np.eye(4)
        
        # åˆ›å»ºæµ‹è¯•æƒé‡
        test_weights = np.random.rand(test_vertices, canonicalizer.num_joints)
        test_weights = test_weights / (np.sum(test_weights, axis=1, keepdims=True) + 1e-8)
        
        print(f"æµ‹è¯•æƒé‡å½¢çŠ¶: {test_weights.shape}")
        print(f"æµ‹è¯•resté¡¶ç‚¹å½¢çŠ¶: {rest_norm.shape}")
        print(f"æµ‹è¯•targeté¡¶ç‚¹å½¢çŠ¶: {target_norm.shape}")
        
        # æµ‹è¯•æŸå¤±å‡½æ•°
        test_weights_flat = test_weights.flatten()
        loss = canonicalizer.compute_lbs_loss(test_weights_flat, rest_norm, target_norm, relative_transforms)
        print(f"æµ‹è¯•æŸå¤±: {loss}")
        
        print("\nâœ… è°ƒè¯•å®Œæˆï¼ç»„ä»¶çœ‹èµ·æ¥æ­£å¸¸å·¥ä½œ")
        return True
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. åŸºæœ¬LBSå˜æ¢æµ‹è¯•")
    print("2. ç®€å•ä¼˜åŒ–æµ‹è¯•")
    print("3. è°ƒè¯•ä¼˜åŒ–é—®é¢˜")
    
    try:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
        
        if choice == "1":
            success = test_basic_lbs()
        elif choice == "2":
            success = test_simple_optimization()
        elif choice == "3":
            success = debug_optimization_issue()
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤è¿è¡ŒåŸºæœ¬æµ‹è¯•")
            success = test_basic_lbs()
        
        if success:
            print("\nğŸ‰ æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\næµ‹è¯•å‡ºé”™: {e}")
        sys.exit(1)
